{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea31859",
   "metadata": {},
   "source": [
    "# üöÄ GPU Setup Guide for Deep Learning\n",
    "\n",
    "## Why GPU Acceleration Matters\n",
    "- **Speed**: Training 5-10x faster than CPU-only\n",
    "- **Scale**: Handle larger models and datasets\n",
    "- **Efficiency**: Better performance per watt\n",
    "\n",
    "## Installation Steps by Operating System\n",
    "\n",
    "### üêß Linux / WSL2\n",
    "1. **Install NVIDIA Drivers**\n",
    "    ```bash\n",
    "    sudo apt update && sudo apt install nvidia-driver-535\n",
    "    ```\n",
    "\n",
    "2. **Install CUDA Toolkit**\n",
    "    ```bash\n",
    "    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb\n",
    "    sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
    "    sudo apt-get update\n",
    "    sudo apt-get -y install cuda-toolkit-12-4\n",
    "    ```\n",
    "\n",
    "3. **Set Environment Variables**\n",
    "    ```bash\n",
    "    echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc\n",
    "    echo 'export PATH=$CUDA_HOME/bin:$PATH' >> ~/.bashrc\n",
    "    echo 'export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
    "    source ~/.bashrc\n",
    "    ```\n",
    "\n",
    "4. **Install cuDNN**: Download from [NVIDIA cuDNN](https://developer.nvidia.com/cudnn)\n",
    "\n",
    "### ü™ü Windows\n",
    "- **Best Option**: Use WSL2 with Ubuntu (follow Linux instructions above)\n",
    "- **Native Windows**: Install CUDA from [NVIDIA Downloads](https://developer.nvidia.com/cuda-downloads)\n",
    "\n",
    "### üçé macOS\n",
    "- Apple Silicon (M1/M2/M3): TensorFlow uses Metal Performance Shaders automatically\n",
    "- Intel Mac: CPU-only mode is your only option\n",
    "\n",
    "## Verification\n",
    "1. Run the **\"Verify System and GPU is OK\"** cell above\n",
    "2. Verify CUDA installation: `nvcc --version`\n",
    "3. Check GPU detection: `nvidia-smi`\n",
    "\n",
    "## Troubleshooting\n",
    "If issues persist, run the **GPU Troubleshooting** cell to diagnose and fix common problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671adf74",
   "metadata": {},
   "source": [
    "```markdown\n",
    "> **‚ÑπÔ∏è To verify your deep learning environment:**  \n",
    "Run the cells below to automatically check if your GPU, CUDA, cuDNN, and TensorFlow are installed and properly detected by your system.  \n",
    "These checks will help you confirm that your hardware and software are ready for accelerated training, and provide troubleshooting tips if any issues are found.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb0be9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 22:54:53.065432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753368893.138026    2969 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753368893.159265    2969 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753368893.331272    2969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753368893.331294    2969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753368893.331295    2969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753368893.331296    2969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Car Classification Environment\n",
      "========================================\n",
      "TensorFlow version: 2.19.0\n",
      "Operating System: Linux\n",
      "Architecture: x86_64\n",
      "üêß Linux detected: Checking for GPU support...\n",
      "GPU devices detected: 1\n",
      "üéâ GPU DETECTED!\n",
      "  üéÆ GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "‚úÖ GPU memory growth enabled\n",
      "‚úÖ Mixed precision enabled for optimal GPU performance\n",
      "‚ö° Enabling CPU optimizations...\n",
      "‚úÖ Environment ready for car classification training!\n",
      "üìä Performance optimized for your hardware configuration\n"
     ]
    }
   ],
   "source": [
    "# Main imports for Car Classification Project\n",
    "import os\n",
    "\n",
    "# Configure TensorFlow environment before importing TF\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING messages\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN warnings\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'  # Better GPU memory management\n",
    "\n",
    "import platform\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"üöÄ Car Classification Environment\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Operating System: {platform.system()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "\n",
    "# Check for GPU availability (OS-dependent)\n",
    "system = platform.system().lower()\n",
    "\n",
    "if system == 'windows':\n",
    "    print(\"ü™ü Windows detected: Running in CPU-only mode\")\n",
    "    print(\"üí° For GPU support on Windows, consider using WSL2 or Docker\")\n",
    "    print(\"üöÄ Your RTX 4060 can still be used via WSL2 if needed\")\n",
    "    \n",
    "elif system == 'linux':\n",
    "    print(\"üêß Linux detected: Checking for GPU support...\")\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"GPU devices detected: {len(gpus)}\")\n",
    "    \n",
    "    if gpus:\n",
    "        print(\"üéâ GPU DETECTED!\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"  üéÆ GPU {i}: {gpu}\")\n",
    "        \n",
    "        # Configure GPU memory growth to prevent allocation errors\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"‚úÖ GPU memory growth enabled\")\n",
    "        except RuntimeError:\n",
    "            print(\"‚ö†Ô∏è GPU memory growth already configured\")\n",
    "        \n",
    "        # Enable mixed precision for better performance\n",
    "        try:\n",
    "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "            print(\"‚úÖ Mixed precision enabled for optimal GPU performance\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Mixed precision setup: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU detected - using CPU\")\n",
    "        print(\"üí° For GPU setup, run the troubleshooting cell below\")\n",
    "        \n",
    "elif system == 'darwin':\n",
    "    print(\"üçé macOS detected: Optimized for Apple hardware\")\n",
    "    if 'arm' in platform.machine().lower():\n",
    "        print(\"üöÄ Apple Silicon detected: Using optimized Metal Performance Shaders\")\n",
    "    else:\n",
    "        print(\"üíª Intel Mac: CPU-only mode\")\n",
    "\n",
    "# CPU optimizations for all platforms\n",
    "print(\"‚ö° Enabling CPU optimizations...\")\n",
    "tf.config.threading.set_inter_op_parallelism_threads(0)  # Use all available cores\n",
    "tf.config.threading.set_intra_op_parallelism_threads(0)  # Use all available cores\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment ready for car classification training!\")\n",
    "print(\"üìä Performance optimized for your hardware configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ab6c5",
   "metadata": {},
   "source": [
    "## Verify System and GPU is OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4027711c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== System Information ===\n",
      "Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "System: Linux\n",
      "Python Version: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n",
      "TensorFlow Version: 2.19.0\n",
      "GPU Available: True\n",
      "Number of GPUs: 1\n",
      "‚úÖ GPU Details:\n",
      "  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "‚úÖ GPU memory growth configured\n",
      "\n",
      "=== CUDA Environment Check ===\n",
      "‚úÖ CUDA found at: /usr/local/cuda/bin/nvcc\n",
      "‚úÖ cuDNN found at: /usr/lib/x86_64-linux-gnu/libcudnn.so\n",
      "\n",
      "=== Performance Optimization ===\n",
      "üéÆ GPU acceleration enabled\n",
      "‚úÖ Mixed precision enabled for optimal GPU performance\n",
      "\n",
      "=== Environment Status ===\n",
      "üéâ GPU setup complete!\n",
      "üìà Training will use GPU acceleration\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# System verification and comprehensive GPU troubleshooting\n",
    "import platform\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"System: {platform.system()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU Available: {len(physical_devices) > 0}\")\n",
    "print(f\"Number of GPUs: {len(physical_devices)}\")\n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"‚úÖ GPU Details:\")\n",
    "    for i, device in enumerate(physical_devices):\n",
    "        print(f\"  GPU {i}: {device}\")\n",
    "        \n",
    "    # Configure GPU memory growth to avoid allocation errors\n",
    "    try:\n",
    "        for gpu in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ GPU memory growth configured\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è GPU memory configuration failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected\")\n",
    "    print(\"\\nüîß GPU Troubleshooting Steps:\")\n",
    "    print(\"1. Check if NVIDIA GPU is available:\")\n",
    "    print(\"   Run: nvidia-smi\")\n",
    "    print(\"2. Install CUDA Toolkit:\")\n",
    "    print(\"   Visit: https://developer.nvidia.com/cuda-downloads\")\n",
    "    print(\"3. Install cuDNN:\")\n",
    "    print(\"   Visit: https://developer.nvidia.com/cudnn\")\n",
    "    print(\"4. Verify CUDA installation:\")\n",
    "    print(\"   Run: nvcc --version\")\n",
    "    print(\"5. Check TensorFlow GPU installation:\")\n",
    "    print(\"   Run: python -c 'import tensorflow as tf; print(tf.config.list_physical_devices())'\")\n",
    "\n",
    "print(\"\\n=== CUDA Environment Check ===\")\n",
    "# Check for CUDA installation\n",
    "cuda_paths = [\n",
    "    \"/usr/local/cuda/bin/nvcc\",\n",
    "    \"/usr/bin/nvcc\",\n",
    "    \"/opt/cuda/bin/nvcc\"\n",
    "]\n",
    "\n",
    "cuda_found = False\n",
    "for path in cuda_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ CUDA found at: {path}\")\n",
    "        cuda_found = True\n",
    "        break\n",
    "\n",
    "if not cuda_found:\n",
    "    print(\"‚ùå CUDA not found in common locations\")\n",
    "    print(\"üí° Install CUDA from: https://developer.nvidia.com/cuda-downloads\")\n",
    "\n",
    "# Check for cuDNN\n",
    "# Use existing cudnn_paths variable if present, otherwise define common locations\n",
    "cudnn_paths = []\n",
    "if 'cudnn_paths' not in globals():\n",
    "    cudnn_paths = [\n",
    "        \"/usr/local/cuda/include/cudnn.h\",\n",
    "        \"/usr/include/cudnn.h\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn.so\",\n",
    "        \"/usr/lib/x86_64-linux-gnu/libcudnn.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_ops_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_ops_train.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_adv_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_adv_train.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_cnn_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_cnn_train.so\",\n",
    "        \"/usr/lib/x86_64-linux-gnu/include/cudnn.h\",\n",
    "        \"/usr/lib64/include/cudnn.h\"\n",
    "    ]\n",
    "else:\n",
    "    # Optionally, extend the existing cudnn_paths with more locations\n",
    "    cudnn_paths.extend([\n",
    "        \"/usr/local/cuda/lib64/libcudnn.so\",\n",
    "        \"/usr/lib/x86_64-linux-gnu/libcudnn.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_ops_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_ops_train.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_adv_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_adv_train.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_cnn_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_cnn_train.so\",\n",
    "        \"/usr/lib/x86_64-linux-gnu/include/cudnn.h\",\n",
    "        \"/usr/lib64/include/cudnn.h\"\n",
    "    ])\n",
    "\n",
    "cudnn_found = False\n",
    "for path in cudnn_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ cuDNN found at: {path}\")\n",
    "        cudnn_found = True\n",
    "        break\n",
    "\n",
    "if not cudnn_found:\n",
    "    print(\"‚ùå cuDNN not found\")\n",
    "    print(\"üí° Install cuDNN from: https://developer.nvidia.com/cudnn\")\n",
    "\n",
    "print(\"\\n=== Performance Optimization ===\")\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"üéÆ GPU acceleration enabled\")\n",
    "    # Enable mixed precision for better GPU performance\n",
    "    try:\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "        print(\"‚úÖ Mixed precision enabled for optimal GPU performance\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Mixed precision setup failed: {e}\")\n",
    "else:\n",
    "    print(\"üíª CPU-only mode - optimizing for CPU performance\")\n",
    "    # CPU optimizations\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(0)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(0)\n",
    "    print(\"‚úÖ CPU threading optimized for maximum performance\")\n",
    "\n",
    "print(\"\\n=== Environment Status ===\")\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"üéâ GPU setup complete!\")\n",
    "    print(\"üìà Training will use GPU acceleration\")\n",
    "else:\n",
    "    print(\"‚úÖ CPU setup complete!\")\n",
    "    print(\"üìä Training will use optimized CPU performance\")\n",
    "    print(\"‚è±Ô∏è Expected training time: 2-3x longer than GPU (still manageable)\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7edbf4b",
   "metadata": {},
   "source": [
    "## If there exists GPU issue, please run the cell below to troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4a7293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç GPU Troubleshooting Assistant\n",
      "========================================\n",
      "\n",
      "1Ô∏è‚É£ Checking NVIDIA GPU...\n",
      "‚úÖ NVIDIA GPU detected:\n",
      "   üéÆ |   0  NVIDIA GeForce RTX 4060        On  |   00000000:01:00.0  On |                  N/A |\n",
      "\n",
      "2Ô∏è‚É£ Checking CUDA installation...\n",
      "‚úÖ CUDA installed:\n",
      "   üì¶ Cuda compilation tools, release 12.9, V12.9.86\n",
      "\n",
      "3Ô∏è‚É£ Checking CUDA environment...\n",
      "   CUDA_HOME: /usr/local/cuda\n",
      "\n",
      "4Ô∏è‚É£ Checking TensorFlow GPU support...\n",
      "‚úÖ TensorFlow can see 1 GPU(s)\n",
      "   üéÆ GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "‚úÖ GPU computation test successful\n",
      "\n",
      "5Ô∏è‚É£ Quick Fix Commands (if needed):\n",
      "   # Install NVIDIA drivers\n",
      "   sudo apt update && sudo apt install nvidia-driver-535\n",
      "   \n",
      "   # Install CUDA (Ubuntu 24.04)\n",
      "   wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb\n",
      "   sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
      "   sudo apt-get update\n",
      "   sudo apt-get -y install cuda-toolkit-12-4\n",
      "   \n",
      "   # Add to ~/.bashrc\n",
      "   echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc\n",
      "   echo 'export PATH=$CUDA_HOME/bin:$PATH' >> ~/.bashrc\n",
      "   echo 'export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
      "   source ~/.bashrc\n",
      "\n",
      "========================================\n",
      "üîÑ After installing CUDA, restart your kernel: Kernel ‚Üí Restart Kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753368896.613146    2969 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1753368896.615040    2969 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# üîß GPU Troubleshooting and Setup Assistant\n",
    "# Run this cell if you're experiencing GPU issues\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_command(command, description):\n",
    "    \"\"\"Run a system command and return the result\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            return True, result.stdout.strip()\n",
    "        else:\n",
    "            return False, result.stderr.strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, \"Command timed out\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"üîç GPU Troubleshooting Assistant\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Check NVIDIA GPU\n",
    "print(\"\\n1Ô∏è‚É£ Checking NVIDIA GPU...\")\n",
    "success, output = run_command(\"nvidia-smi\", \"NVIDIA GPU check\")\n",
    "if success:\n",
    "    print(\"‚úÖ NVIDIA GPU detected:\")\n",
    "    # Extract GPU info from nvidia-smi output\n",
    "    lines = output.split('\\n')\n",
    "    for line in lines:\n",
    "        if 'RTX' in line or 'GTX' in line or 'Tesla' in line or 'GeForce' in line:\n",
    "            print(f\"   üéÆ {line.strip()}\")\n",
    "else:\n",
    "    print(\"‚ùå NVIDIA GPU not detected or nvidia-smi not available\")\n",
    "    print(\"üí° Possible solutions:\")\n",
    "    print(\"   - Install NVIDIA drivers: sudo apt update && sudo apt install nvidia-driver-xxx\")\n",
    "    print(\"   - Check if GPU is properly connected\")\n",
    "    print(\"   - Restart your system after driver installation\")\n",
    "\n",
    "# 2. Check CUDA installation\n",
    "print(\"\\n2Ô∏è‚É£ Checking CUDA installation...\")\n",
    "success, output = run_command(\"nvcc --version\", \"CUDA compiler check\")\n",
    "if success:\n",
    "    print(\"‚úÖ CUDA installed:\")\n",
    "    for line in output.split('\\n'):\n",
    "        if 'release' in line.lower():\n",
    "            print(f\"   üì¶ {line.strip()}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not installed or not in PATH\")\n",
    "    print(\"üí° Install CUDA:\")\n",
    "    print(\"   Ubuntu 24.04: https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=24.04&target_type=deb_network\")\n",
    "\n",
    "# 3. Check CUDA environment variables\n",
    "print(\"\\n3Ô∏è‚É£ Checking CUDA environment...\")\n",
    "cuda_home = os.environ.get('CUDA_HOME', os.environ.get('CUDA_PATH', 'Not set'))\n",
    "ld_library_path = os.environ.get('LD_LIBRARY_PATH', 'Not set')\n",
    "\n",
    "print(f\"   CUDA_HOME: {cuda_home}\")\n",
    "if 'cuda' not in cuda_home.lower():\n",
    "    print(\"   ‚ö†Ô∏è CUDA_HOME not properly set\")\n",
    "    print(\"   üí° Add to ~/.bashrc: export CUDA_HOME=/usr/local/cuda\")\n",
    "\n",
    "# 4. Check TensorFlow GPU support\n",
    "print(\"\\n4Ô∏è‚É£ Checking TensorFlow GPU support...\")\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Suppress warnings for this check\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    \n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"‚úÖ TensorFlow can see {len(gpus)} GPU(s)\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"   üéÆ GPU {i}: {gpu}\")\n",
    "        \n",
    "        # Test GPU computation\n",
    "        try:\n",
    "            with tf.device('/GPU:0'):\n",
    "                a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "                b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "                c = tf.matmul(a, b)\n",
    "            print(\"‚úÖ GPU computation test successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå GPU computation test failed: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå TensorFlow cannot detect GPU\")\n",
    "        print(\"üí° Possible solutions:\")\n",
    "        print(\"   - Reinstall TensorFlow with GPU support: pip install tensorflow[and-cuda]\")\n",
    "        print(\"   - Check CUDA compatibility: https://www.tensorflow.org/install/source#gpu\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå TensorFlow import failed: {e}\")\n",
    "\n",
    "# 5. Provide installation commands\n",
    "print(\"\\n5Ô∏è‚É£ Quick Fix Commands (if needed):\")\n",
    "print(\"   # Install NVIDIA drivers\")\n",
    "print(\"   sudo apt update && sudo apt install nvidia-driver-535\")\n",
    "print(\"   \")\n",
    "print(\"   # Install CUDA (Ubuntu 24.04)\")\n",
    "print(\"   wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb\")\n",
    "print(\"   sudo dpkg -i cuda-keyring_1.1-1_all.deb\")\n",
    "print(\"   sudo apt-get update\")\n",
    "print(\"   sudo apt-get -y install cuda-toolkit-12-4\")\n",
    "print(\"   \")\n",
    "print(\"   # Add to ~/.bashrc\")\n",
    "print(\"   echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc\")\n",
    "print(\"   echo 'export PATH=$CUDA_HOME/bin:$PATH' >> ~/.bashrc\")\n",
    "print(\"   echo 'export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\")\n",
    "print(\"   source ~/.bashrc\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"üîÑ After installing CUDA, restart your kernel: Kernel ‚Üí Restart Kernel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
