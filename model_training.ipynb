{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1fbe9b",
   "metadata": {},
   "source": [
    "# Car Type Classification - Assignment\n",
    "\n",
    "## System Information\n",
    "- **Development Environment**: WSL2 Ubuntu on Windows\n",
    "- **Python Version**: 3.11+\n",
    "- **TensorFlow Version**: 2.19.0 with GPU support\n",
    "- **Platform**: Linux (WSL2) - Recommended for optimal performance\n",
    "\n",
    "## Overview\n",
    "This notebook implements a car type classification system using TensorFlow 2.19 and the Stanford Cars dataset. It automatically checks for pre-trained models and skips training if available.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d60b302",
   "metadata": {},
   "source": [
    "# 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd938855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 23:29:36.149005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753198176.220165     821 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753198176.241410     821 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753198176.402448     821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753198176.402469     821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753198176.402470     821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753198176.402471     821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== System Information ===\n",
      "Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "Python Version: 3.12.3\n",
      "TensorFlow Version: 2.19.0\n",
      "✅ GPU Available: 1 device(s)\n",
      "✅ GPU memory growth configured\n",
      "✅ Mixed precision enabled\n",
      "\n",
      "✅ Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ENVIRONMENT SETUP\n",
    "# ==========================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "# Core ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset handling\n",
    "import kagglehub\n",
    "\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Configure GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(f\"✅ GPU Available: {len(physical_devices)} device(s)\")\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"✅ GPU memory growth configured\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected - using CPU\")\n",
    "\n",
    "# Set mixed precision for better performance\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"✅ Mixed precision enabled\")\n",
    "\n",
    "print(\"\\n✅ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58218926",
   "metadata": {},
   "source": [
    "# 2. Dataset Configuration and Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9c8a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DETECTING STANFORD CARS DATASET...\n",
      "=============================================\n",
      "📁 Checking: /home/alph/Car-Type-Classification-Service/stanford-cars-by-classes-folder\n",
      "✅ Found dataset: 196 classes\n",
      "\n",
      "✅ DATASET READY!\n",
      "📊 Classes: 196\n",
      "🖼️ Training images: 8144\n",
      "🖼️ Test images: 8041\n",
      "📁 Train dir: /home/alph/Car-Type-Classification-Service/stanford-cars-by-classes-folder/train\n",
      "📁 Test dir: /home/alph/Car-Type-Classification-Service/stanford-cars-by-classes-folder/test\n",
      "\n",
      "📋 Configuration:\n",
      "   Classes: 196\n",
      "   Batch size: 32\n",
      "   Image size: (224, 224)\n",
      "   Max epochs: 50\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# DATASET DETECTION\n",
    "# ==========================\n",
    "\n",
    "def detect_stanford_cars_dataset():\n",
    "    \"\"\"\n",
    "    Robustly detect various Stanford Cars dataset folder structures\n",
    "    \"\"\"\n",
    "    print(\"🔍 DETECTING STANFORD CARS DATASET...\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Possible dataset locations\n",
    "    search_paths = [\n",
    "        Path.cwd() / \"stanford-cars-by-classes-folder\",\n",
    "        Path.cwd() / \"data\", \n",
    "        Path.cwd(),\n",
    "        Path.home() / \".cache\" / \"kagglehub\" / \"datasets\" / \"cyizhuo\" / \"stanford-cars-by-classes-folder\"\n",
    "    ]\n",
    "    \n",
    "    dataset_ready = False\n",
    "    train_dir = None\n",
    "    test_dir = None\n",
    "    \n",
    "    # Check existing locations\n",
    "    for base_path in search_paths:\n",
    "        if base_path.exists():\n",
    "            print(f\"📁 Checking: {base_path}\")\n",
    "            \n",
    "            # Look for train/test folders\n",
    "            possible_structures = [\n",
    "                (base_path / \"train\", base_path / \"test\"),\n",
    "                (base_path / \"stanford-cars-by-classes-folder\" / \"train\", \n",
    "                 base_path / \"stanford-cars-by-classes-folder\" / \"test\"),\n",
    "                # Handle nested versions folder\n",
    "                (base_path / \"versions\" / \"5\" / \"train\", base_path / \"versions\" / \"5\" / \"test\"),\n",
    "            ]\n",
    "            \n",
    "            for train_candidate, test_candidate in possible_structures:\n",
    "                if train_candidate.exists() and test_candidate.exists():\n",
    "                    # Verify it contains car classes\n",
    "                    train_classes = [d for d in train_candidate.iterdir() if d.is_dir()]\n",
    "                    test_classes = [d for d in test_candidate.iterdir() if d.is_dir()]\n",
    "                    \n",
    "                    if len(train_classes) >= 150:  # Should be ~196 classes\n",
    "                        train_dir = train_candidate\n",
    "                        test_dir = test_candidate\n",
    "                        dataset_ready = True\n",
    "                        print(f\"✅ Found dataset: {len(train_classes)} classes\")\n",
    "                        break\n",
    "            \n",
    "            if dataset_ready:\n",
    "                break\n",
    "    \n",
    "    # If not found, try to download\n",
    "    if not dataset_ready:\n",
    "        print(\"📥 Dataset not found locally, downloading...\")\n",
    "        try:\n",
    "            download_path = kagglehub.dataset_download(\"cyizhuo/stanford-cars-by-classes-folder\")\n",
    "            download_path = Path(download_path)\n",
    "            \n",
    "            # Check downloaded structure\n",
    "            if (download_path / \"train\").exists():\n",
    "                train_dir = download_path / \"train\"\n",
    "                test_dir = download_path / \"test\"\n",
    "                dataset_ready = True\n",
    "                print(f\"✅ Downloaded to: {download_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Download failed: {e}\")\n",
    "    \n",
    "    # Results\n",
    "    if dataset_ready:\n",
    "        train_count = len(list(train_dir.glob(\"*/*.jpg\")))\n",
    "        test_count = len(list(test_dir.glob(\"*/*.jpg\")))\n",
    "        class_count = len([d for d in train_dir.iterdir() if d.is_dir()])\n",
    "        \n",
    "        print(f\"\\n✅ DATASET READY!\")\n",
    "        print(f\"📊 Classes: {class_count}\")\n",
    "        print(f\"🖼️ Training images: {train_count}\")\n",
    "        print(f\"🖼️ Test images: {test_count}\")\n",
    "        print(f\"📁 Train dir: {train_dir}\")\n",
    "        print(f\"📁 Test dir: {test_dir}\")\n",
    "    else:\n",
    "        print(\"❌ Stanford Cars dataset not available\")\n",
    "        print(\"💡 Please download manually or check internet connection\")\n",
    "    \n",
    "    return dataset_ready, train_dir, test_dir\n",
    "\n",
    "# Detect dataset\n",
    "DATASET_READY, TRAIN_DIR, TEST_DIR = detect_stanford_cars_dataset()\n",
    "USING_REAL_DATASET = DATASET_READY\n",
    "\n",
    "# Set global parameters\n",
    "if DATASET_READY:\n",
    "    NUM_CLASSES = len([d for d in TRAIN_DIR.iterdir() if d.is_dir()])\n",
    "else:\n",
    "    NUM_CLASSES = 196  # Default for Stanford Cars\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "EPOCHS = 50\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "print(f\"\\n📋 Configuration:\")\n",
    "print(f\"   Classes: {NUM_CLASSES}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Image size: {IMG_SIZE}\")\n",
    "print(f\"   Max epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9987e5",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1056507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Creating data pipeline with TensorFlow 2.19...\n",
      "Found 8144 files belonging to 196 classes.\n",
      "Using 6516 files for training.\n",
      "Found 8144 files belonging to 196 classes.\n",
      "Using 6516 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753198180.262720     821 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8144 files belonging to 196 classes.\n",
      "Using 1628 files for validation.\n",
      "Using 1628 files for validation.\n",
      "Found 8041 files belonging to 196 classes.\n",
      "Found 8041 files belonging to 196 classes.\n",
      "✅ Data pipeline created:\n",
      "   Training batches: 204\n",
      "   Validation batches: 51\n",
      "   Test batches: 252\n",
      "\n",
      "📊 Data pipeline ready for training!\n",
      "✅ Data pipeline created:\n",
      "   Training batches: 204\n",
      "   Validation batches: 51\n",
      "   Test batches: 252\n",
      "\n",
      "📊 Data pipeline ready for training!\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# DATA PREPROCESSING\n",
    "# ==========================\n",
    "\n",
    "def create_data_pipeline():\n",
    "    \"\"\"\n",
    "    Create TensorFlow 2.19 data pipeline with tf.data.Dataset\n",
    "    \"\"\"\n",
    "    if not DATASET_READY:\n",
    "        print(\"⚠️ Real dataset not available - using demo data\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(\"🔧 Creating data pipeline with TensorFlow 2.19...\")\n",
    "    \n",
    "    # Training dataset with validation split\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    # Validation dataset\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\", \n",
    "        seed=123,\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    # Test dataset\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TEST_DIR,\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Data augmentation for training\n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "        layers.RandomContrast(0.1),\n",
    "    ])\n",
    "    \n",
    "    # Apply augmentation to training data\n",
    "    train_ds = train_ds.map(\n",
    "        lambda x, y: (data_augmentation(x, training=True), y),\n",
    "        num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Optimize datasets for performance\n",
    "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    print(f\"✅ Data pipeline created:\")\n",
    "    print(f\"   Training batches: {len(train_ds)}\")\n",
    "    print(f\"   Validation batches: {len(val_ds)}\")\n",
    "    print(f\"   Test batches: {len(test_ds)}\")\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "# Create data pipeline\n",
    "if DATASET_READY:\n",
    "    train_ds, val_ds, test_ds = create_data_pipeline()\n",
    "    print(\"\\n📊 Data pipeline ready for training!\")\n",
    "else:\n",
    "    train_ds = val_ds = test_ds = None\n",
    "    print(\"\\n⚠️ Data pipeline skipped - no dataset available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b2889",
   "metadata": {},
   "source": [
    "# 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7317e5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Found existing trained model!\n",
      "📁 Loading model from: best_car_model.keras\n",
      "✅ Pre-trained model loaded successfully!\n",
      "📊 Model has 24,820,548 parameters\n",
      "⚙️ Training callbacks configured\n",
      "\n",
      "🎯 Model ready! Skip training: True\n",
      "✅ Pre-trained model loaded successfully!\n",
      "📊 Model has 24,820,548 parameters\n",
      "⚙️ Training callbacks configured\n",
      "\n",
      "🎯 Model ready! Skip training: True\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# MODEL ARCHITECTURE\n",
    "# ==========================\n",
    "\n",
    "def create_car_classification_model():\n",
    "    \"\"\"\n",
    "    Create ResNet50-based model optimized for TensorFlow 2.19\n",
    "    \"\"\"\n",
    "    print(\"🏗️ Creating car classification model...\")\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=(*IMG_SIZE, 3))\n",
    "    \n",
    "    # Preprocessing\n",
    "    x = layers.Rescaling(1./255)(inputs)\n",
    "    \n",
    "    # Pre-trained ResNet50 backbone\n",
    "    backbone = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_tensor=x\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers, unfreeze later ones for fine-tuning\n",
    "    backbone.trainable = True\n",
    "    for layer in backbone.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Classification head\n",
    "    x = backbone.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Output layer - explicit float32 for mixed precision compatibility\n",
    "    predictions = layers.Dense(\n",
    "        NUM_CLASSES, \n",
    "        activation='softmax',\n",
    "        dtype='float32',  # Important for mixed precision\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "        name='predictions'\n",
    "    )(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(inputs, predictions, name='car_classifier')\n",
    "    \n",
    "    # Compile with TF 2.19 compatible metrics\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy', dtype='float32')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Model created and compiled successfully!\")\n",
    "    print(f\"📊 Total parameters: {model.count_params():,}\")\n",
    "    print(f\"📊 Trainable parameters: {sum(p.numel() for p in model.trainable_weights):,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_callbacks():\n",
    "    \"\"\"\n",
    "    Setup training callbacks for TF 2.19\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_car_model.keras',  # TF 2.19 .keras format\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"⚙️ Training callbacks configured\")\n",
    "    return callbacks\n",
    "\n",
    "# Check if pre-trained model exists\n",
    "model_path = Path('best_car_model.keras')\n",
    "if model_path.exists():\n",
    "    print(\"🔍 Found existing trained model!\")\n",
    "    print(f\"📁 Loading model from: {model_path}\")\n",
    "    try:\n",
    "        model = keras.models.load_model(str(model_path))\n",
    "        print(\"✅ Pre-trained model loaded successfully!\")\n",
    "        print(f\"📊 Model has {model.count_params():,} parameters\")\n",
    "        SKIP_TRAINING = True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load model: {e}\")\n",
    "        print(\"🔧 Creating new model instead...\")\n",
    "        model = create_car_classification_model()\n",
    "        SKIP_TRAINING = False\n",
    "else:\n",
    "    print(\"📋 No existing model found - creating new model\")\n",
    "    model = create_car_classification_model()\n",
    "    SKIP_TRAINING = False\n",
    "\n",
    "# Setup callbacks regardless (needed for evaluation)\n",
    "callbacks = setup_callbacks()\n",
    "\n",
    "print(f\"\\n🎯 Model ready! Skip training: {SKIP_TRAINING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f7b8f",
   "metadata": {},
   "source": [
    "# 5. Model Training (Conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772c9d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 SKIPPING TRAINING - Pre-trained model available\n",
      "==================================================\n",
      "✅ Using existing model: best_car_model.keras\n",
      "📊 Model parameters: 24,820,548\n",
      "💡 To retrain, delete the .keras file and rerun this cell\n",
      "\n",
      "📋 Training Status: Skipped (model exists)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# MODEL TRAINING (CONDITIONAL)\n",
    "# ==========================\n",
    "\n",
    "if SKIP_TRAINING:\n",
    "    print(\"🔄 SKIPPING TRAINING - Pre-trained model available\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"✅ Using existing model: best_car_model.keras\")\n",
    "    print(f\"📊 Model parameters: {model.count_params():,}\")\n",
    "    print(\"💡 To retrain, delete the .keras file and rerun this cell\")\n",
    "    history = None\n",
    "    \n",
    "elif not DATASET_READY:\n",
    "    print(\"⚠️ SKIPPING TRAINING - No dataset available\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"❌ Cannot train without Stanford Cars dataset\")\n",
    "    print(\"💡 Please ensure dataset is available and rerun\")\n",
    "    history = None\n",
    "    \n",
    "else:\n",
    "    print(\"🚀 STARTING MODEL TRAINING\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"📊 Training on {NUM_CLASSES} car classes\")\n",
    "    print(f\"🎯 Max epochs: {EPOCHS} (early stopping enabled)\")\n",
    "    print(f\"⚡ Using mixed precision for optimal performance\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Training completed successfully!\")\n",
    "    print(f\"💾 Best model saved as: best_car_model.keras\")\n",
    "    \n",
    "    # Plot training history\n",
    "    if history:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Accuracy\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Training')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(history.history['loss'], label='Training')\n",
    "        plt.plot(history.history['val_loss'], label='Validation')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Top-5 Accuracy\n",
    "        if 'top_5_accuracy' in history.history:\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(history.history['top_5_accuracy'], label='Training')\n",
    "            plt.plot(history.history['val_top_5_accuracy'], label='Validation')\n",
    "            plt.title('Top-5 Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(f\"\\n📋 Training Status: {'Skipped (model exists)' if SKIP_TRAINING else 'Completed' if DATASET_READY else 'Skipped (no dataset)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c443ff",
   "metadata": {},
   "source": [
    "# 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a896e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 EVALUATING MODEL PERFORMANCE\n",
      "===================================\n",
      "🎯 Validation Set Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753198184.973795     997 service.cc:152] XLA service 0x754d40002860 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753198184.973831     997 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060, Compute Capability 8.9\n",
      "I0000 00:00:1753198185.581472     997 cuda_dnn.cc:529] Loaded cuDNN version 91100\n",
      "I0000 00:00:1753198185.581472     997 cuda_dnn.cc:529] Loaded cuDNN version 91100\n",
      "I0000 00:00:1753198190.945880     997 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "I0000 00:00:1753198190.945880     997 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loss: 2.1096\n",
      "   Accuracy: 0.5405 (54.05%)\n",
      "   Top-5 Accuracy: 0.7869 (78.69%)\n",
      "\n",
      "🎯 Test Set Results:\n",
      "   Loss: 2.1031\n",
      "   Accuracy: 0.5381 (53.81%)\n",
      "   Top-5 Accuracy: 0.7855 (78.55%)\n",
      "\n",
      "✅ Model evaluation completed!\n",
      "   Loss: 2.1031\n",
      "   Accuracy: 0.5381 (53.81%)\n",
      "   Top-5 Accuracy: 0.7855 (78.55%)\n",
      "\n",
      "✅ Model evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# MODEL EVALUATION\n",
    "# ==========================\n",
    "\n",
    "def evaluate_model():\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation\n",
    "    \"\"\"\n",
    "    print(\"📊 EVALUATING MODEL PERFORMANCE\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    if not DATASET_READY:\n",
    "        print(\"⚠️ No dataset available for evaluation\")\n",
    "        return\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"🎯 Validation Set Results:\")\n",
    "    val_results = model.evaluate(val_ds, verbose=0, return_dict=True)\n",
    "    print(f\"   Loss: {val_results['loss']:.4f}\")\n",
    "    print(f\"   Accuracy: {val_results['accuracy']:.4f} ({val_results['accuracy']*100:.2f}%)\")\n",
    "    if 'top_5_accuracy' in val_results:\n",
    "        print(f\"   Top-5 Accuracy: {val_results['top_5_accuracy']:.4f} ({val_results['top_5_accuracy']*100:.2f}%)\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\n🎯 Test Set Results:\")\n",
    "    test_results = model.evaluate(test_ds, verbose=0, return_dict=True)\n",
    "    print(f\"   Loss: {test_results['loss']:.4f}\")\n",
    "    print(f\"   Accuracy: {test_results['accuracy']:.4f} ({test_results['accuracy']*100:.2f}%)\")\n",
    "    if 'top_5_accuracy' in test_results:\n",
    "        print(f\"   Top-5 Accuracy: {test_results['top_5_accuracy']:.4f} ({test_results['top_5_accuracy']*100:.2f}%)\")\n",
    "    \n",
    "    return val_results, test_results\n",
    "\n",
    "# Run evaluation\n",
    "if DATASET_READY:\n",
    "    val_results, test_results = evaluate_model()\n",
    "    print(\"\\n✅ Model evaluation completed!\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping evaluation - no dataset available\")\n",
    "    print(\"💡 Model is ready for inference with external data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360ccce",
   "metadata": {},
   "source": [
    "# 7. Model Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b59b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 FINAL MODEL SUMMARY\n",
      "=========================\n",
      "🏗️ Architecture: ResNet50-based transfer learning\n",
      "📊 Total parameters: 24,820,548\n",
      "🎯 Output classes: 196\n",
      "📐 Input shape: (224, 224, 3)\n",
      "⚡ Mixed precision: Enabled\n",
      "\n",
      "💾 Model Files:\n",
      "   ✅ best_car_model.keras (250.4MB) - Main model file (TensorFlow 2.19 format)\n",
      "   ✅ class_mapping.json (0.0MB) - Class name mappings\n",
      "\n",
      "🚀 USAGE INSTRUCTIONS:\n",
      "   1. Load model: tf.keras.models.load_model('best_car_model.keras')\n",
      "   2. Preprocess image: Resize to (224, 224), normalize to [0,1]\n",
      "   3. Predict: model.predict(preprocessed_image)\n",
      "   4. Get class name: Use class_mapping.json for label lookup\n",
      "\n",
      "✅ Model ready for deployment and inference!\n",
      "\n",
      "🏗️ Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ predictions_fixed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,372</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,049,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ predictions_fixed (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m)            │        \u001b[38;5;34m50,372\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,146,834</span> (172.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,146,834\u001b[0m (172.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,163,140</span> (38.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,163,140\u001b[0m (38.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,657,408</span> (55.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,657,408\u001b[0m (55.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,326,286</span> (77.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m20,326,286\u001b[0m (77.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================\n",
    "# MODEL SUMMARY AND EXPORT\n",
    "# ==========================\n",
    "\n",
    "print(\"📋 FINAL MODEL SUMMARY\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Model architecture summary\n",
    "print(f\"🏗️ Architecture: ResNet50-based transfer learning\")\n",
    "print(f\"📊 Total parameters: {model.count_params():,}\")\n",
    "print(f\"🎯 Output classes: {NUM_CLASSES}\")\n",
    "print(f\"📐 Input shape: {IMG_SIZE + (3,)}\")\n",
    "print(f\"⚡ Mixed precision: Enabled\")\n",
    "\n",
    "# Model files\n",
    "model_files = {\n",
    "    'best_car_model.keras': 'Main model file (TensorFlow 2.19 format)',\n",
    "    'class_mapping.json': 'Class name mappings',\n",
    "}\n",
    "\n",
    "print(f\"\\n💾 Model Files:\")\n",
    "for filename, description in model_files.items():\n",
    "    if Path(filename).exists():\n",
    "        size = Path(filename).stat().st_size / (1024*1024)  # MB\n",
    "        print(f\"   ✅ {filename} ({size:.1f}MB) - {description}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {filename} - {description} (missing)\")\n",
    "\n",
    "# Create class mapping if dataset available\n",
    "if DATASET_READY and not Path('class_mapping.json').exists():\n",
    "    print(\"\\n📝 Creating class mapping...\")\n",
    "    class_names = sorted([d.name for d in TRAIN_DIR.iterdir() if d.is_dir()])\n",
    "    class_mapping = {i: name for i, name in enumerate(class_names)}\n",
    "    \n",
    "    with open('class_mapping.json', 'w') as f:\n",
    "        json.dump(class_mapping, f, indent=2)\n",
    "    print(f\"✅ Class mapping saved with {len(class_mapping)} classes\")\n",
    "\n",
    "# Usage instructions\n",
    "print(f\"\\n🚀 USAGE INSTRUCTIONS:\")\n",
    "print(f\"   1. Load model: tf.keras.models.load_model('best_car_model.keras')\")\n",
    "print(f\"   2. Preprocess image: Resize to {IMG_SIZE}, normalize to [0,1]\")\n",
    "print(f\"   3. Predict: model.predict(preprocessed_image)\")\n",
    "print(f\"   4. Get class name: Use class_mapping.json for label lookup\")\n",
    "\n",
    "print(f\"\\n✅ Model ready for deployment and inference!\")\n",
    "\n",
    "# Display model architecture\n",
    "print(f\"\\n🏗️ Model Architecture:\")\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
