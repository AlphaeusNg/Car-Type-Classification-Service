{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4add6fe",
   "metadata": {},
   "source": [
    "# Car Type Classification - Assignment\n",
    "\n",
    "## System Information\n",
    "- **Development Environment**: WSL2 Ubuntu on Windows\n",
    "- **Python Version**: 3.12\n",
    "- **TensorFlow Version**: 2.19.0 with GPU support\n",
    "- **Platform**: Linux (WSL2) - Recommended for optimal performance\n",
    "\n",
    "## Setup Notes\n",
    "This notebook was developed and tested on WSL2 Ubuntu for the best TensorFlow performance with GPU acceleration. For setup instructions for different platforms (Windows native, Linux, WSL2), please refer to the README.md file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66938ad",
   "metadata": {},
   "source": [
    "# 1. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992a67f",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Set up a Python environment with all necessary dependencies to support data processing, model training, and API development.\n",
    "\n",
    "**Thought process: To ensure efficient training, especially for large datasets like Stanford Cars, we need GPU acceleration. WSL2 on Windows is chosen for its Linux environment, optimal for TensorFlow 2.19.0 with GPU support. We'll verify TensorFlow version and GPU availability to confirm setup, using libraries like NumPy for data handling and ensuring reproducibility with random seeds.**\n",
    "\n",
    "## Steps\n",
    "\n",
    "**System Environment**: This notebook is being run on WSL2 Ubuntu, which provides optimal performance for TensorFlow with GPU acceleration support.\n",
    "\n",
    "**Python Version**: Python 3.12 is used to ensure compatibility with TensorFlow 2.19.0 and FastAPI.\n",
    "\n",
    "**Framework Choice**: The assessment allows TensorFlow 2.x (Keras) or PyTorch. This implementation uses TensorFlow 2.19.0 with Keras for its robust integration with pre-trained models and excellent GPU support on Linux/WSL2.\n",
    "\n",
    "**Dependencies**: All required libraries are installed via the requirements.txt file optimized for Linux/WSL2. For platform-specific installation instructions, refer to README.md.\n",
    "\n",
    "**GPU Support**: This setup leverages CUDA acceleration when available, significantly improving training performance compared to CPU-only execution.\n",
    "\n",
    "## GPU Setup (Optional but Recommended)\n",
    "\n",
    "For optimal performance with GPU acceleration, install CUDA and cuDNN:\n",
    "\n",
    "### Linux/WSL2 CUDA Installation\n",
    "\n",
    "1. **Visit NVIDIA CUDA Downloads**:\n",
    "   - **Ubuntu 24.04**: https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=24.04&target_type=deb_network\n",
    "   - **Other distributions**: https://developer.nvidia.com/cuda-downloads\n",
    "\n",
    "2. **Install CUDA Toolkit** (follow NVIDIA's instructions):\n",
    "   ```bash\n",
    "   # Example for Ubuntu 24.04 - verify commands on NVIDIA's site\n",
    "   wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb\n",
    "   sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
    "   sudo apt-get update\n",
    "   sudo apt-get -y install cuda-toolkit-12-4\n",
    "   ```\n",
    "\n",
    "3. **Verify installation**:\n",
    "   ```bash\n",
    "   nvidia-smi\n",
    "   nvcc --version\n",
    "   ```\n",
    "\n",
    "4. **Install cuDNN** from https://developer.nvidia.com/cudnn\n",
    "\n",
    "### Windows GPU Support\n",
    "- **Recommended**: Use WSL2 with Ubuntu and follow the Linux instructions above\n",
    "- **Alternative**: Install CUDA for Windows from https://developer.nvidia.com/cuda-downloads?target_os=Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports for Car Classification Project\n",
    "import os\n",
    "\n",
    "# Configure TensorFlow environment before importing TF\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING messages\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN warnings\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'  # Better GPU memory management\n",
    "\n",
    "import platform\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"üöÄ Car Classification Environment\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Operating System: {platform.system()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "\n",
    "# Check for GPU availability (OS-dependent)\n",
    "system = platform.system().lower()\n",
    "\n",
    "if system == 'windows':\n",
    "    print(\"ü™ü Windows detected: Running in CPU-only mode\")\n",
    "    print(\"üí° For GPU support on Windows, consider using WSL2 or Docker\")\n",
    "    print(\"üöÄ Your RTX 4060 can still be used via WSL2 if needed\")\n",
    "    \n",
    "elif system == 'linux':\n",
    "    print(\"üêß Linux detected: Checking for GPU support...\")\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"GPU devices detected: {len(gpus)}\")\n",
    "    \n",
    "    if gpus:\n",
    "        print(\"üéâ GPU DETECTED!\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"  üéÆ GPU {i}: {gpu}\")\n",
    "        \n",
    "        # Configure GPU memory growth to prevent allocation errors\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"‚úÖ GPU memory growth enabled\")\n",
    "        except RuntimeError:\n",
    "            print(\"‚ö†Ô∏è GPU memory growth already configured\")\n",
    "        \n",
    "        # Enable mixed precision for better performance\n",
    "        try:\n",
    "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "            print(\"‚úÖ Mixed precision enabled for optimal GPU performance\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Mixed precision setup: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU detected - using CPU\")\n",
    "        print(\"üí° For GPU setup, run the troubleshooting cell below\")\n",
    "        \n",
    "elif system == 'darwin':\n",
    "    print(\"üçé macOS detected: Optimized for Apple hardware\")\n",
    "    if 'arm' in platform.machine().lower():\n",
    "        print(\"üöÄ Apple Silicon detected: Using optimized Metal Performance Shaders\")\n",
    "    else:\n",
    "        print(\"üíª Intel Mac: CPU-only mode\")\n",
    "\n",
    "# CPU optimizations for all platforms\n",
    "print(\"‚ö° Enabling CPU optimizations...\")\n",
    "tf.config.threading.set_inter_op_parallelism_threads(0)  # Use all available cores\n",
    "tf.config.threading.set_intra_op_parallelism_threads(0)  # Use all available cores\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment ready for car classification training!\")\n",
    "print(\"üìä Performance optimized for your hardware configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ab6c5",
   "metadata": {},
   "source": [
    "## Verify System and GPU is OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System verification and comprehensive GPU troubleshooting\n",
    "import platform\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"System: {platform.system()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU Available: {len(physical_devices) > 0}\")\n",
    "print(f\"Number of GPUs: {len(physical_devices)}\")\n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"‚úÖ GPU Details:\")\n",
    "    for i, device in enumerate(physical_devices):\n",
    "        print(f\"  GPU {i}: {device}\")\n",
    "        \n",
    "    # Configure GPU memory growth to avoid allocation errors\n",
    "    try:\n",
    "        for gpu in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ GPU memory growth configured\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è GPU memory configuration failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected\")\n",
    "    print(\"\\nüîß GPU Troubleshooting Steps:\")\n",
    "    print(\"1. Check if NVIDIA GPU is available:\")\n",
    "    print(\"   Run: nvidia-smi\")\n",
    "    print(\"2. Install CUDA Toolkit:\")\n",
    "    print(\"   Visit: https://developer.nvidia.com/cuda-downloads\")\n",
    "    print(\"3. Install cuDNN:\")\n",
    "    print(\"   Visit: https://developer.nvidia.com/cudnn\")\n",
    "    print(\"4. Verify CUDA installation:\")\n",
    "    print(\"   Run: nvcc --version\")\n",
    "    print(\"5. Check TensorFlow GPU installation:\")\n",
    "    print(\"   Run: python -c 'import tensorflow as tf; print(tf.config.list_physical_devices())'\")\n",
    "\n",
    "print(\"\\n=== CUDA Environment Check ===\")\n",
    "# Check for CUDA installation\n",
    "cuda_paths = [\n",
    "    \"/usr/local/cuda/bin/nvcc\",\n",
    "    \"/usr/bin/nvcc\",\n",
    "    \"/opt/cuda/bin/nvcc\"\n",
    "]\n",
    "\n",
    "cuda_found = False\n",
    "for path in cuda_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ CUDA found at: {path}\")\n",
    "        cuda_found = True\n",
    "        break\n",
    "\n",
    "if not cuda_found:\n",
    "    print(\"‚ùå CUDA not found in common locations\")\n",
    "    print(\"üí° Install CUDA from: https://developer.nvidia.com/cuda-downloads\")\n",
    "\n",
    "# Check for cuDNN\n",
    "# Use existing cudnn_paths variable if present, otherwise define common locations\n",
    "cudnn_paths = []\n",
    "if 'cudnn_paths' not in globals():\n",
    "    cudnn_paths = [\n",
    "        \"/usr/local/cuda/include/cudnn.h\",\n",
    "        \"/usr/include/cudnn.h\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn.so\",\n",
    "        \"/usr/lib/x86_64-linux-gnu/libcudnn.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_ops_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_ops_train.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_adv_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_adv_train.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_cnn_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_cnn_train.so\",\n",
    "        \"/usr/lib/x86_64-linux-gnu/include/cudnn.h\",\n",
    "        \"/usr/lib64/include/cudnn.h\"\n",
    "    ]\n",
    "else:\n",
    "    # Optionally, extend the existing cudnn_paths with more locations\n",
    "    cudnn_paths.extend([\n",
    "        \"/usr/local/cuda/lib64/libcudnn.so\",\n",
    "        \"/usr/lib/x86_64-linux-gnu/libcudnn.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_ops_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_ops_train.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_adv_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_adv_train.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_cnn_infer.so\",\n",
    "        \"/usr/local/cuda/lib64/libcudnn_cnn_train.so\",\n",
    "        \"/usr/lib/x86_64-linux-gnu/include/cudnn.h\",\n",
    "        \"/usr/lib64/include/cudnn.h\"\n",
    "    ])\n",
    "\n",
    "cudnn_found = False\n",
    "for path in cudnn_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ cuDNN found at: {path}\")\n",
    "        cudnn_found = True\n",
    "        break\n",
    "\n",
    "if not cudnn_found:\n",
    "    print(\"‚ùå cuDNN not found\")\n",
    "    print(\"üí° Install cuDNN from: https://developer.nvidia.com/cudnn\")\n",
    "\n",
    "print(\"\\n=== Performance Optimization ===\")\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"üéÆ GPU acceleration enabled\")\n",
    "    # Enable mixed precision for better GPU performance\n",
    "    try:\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "        print(\"‚úÖ Mixed precision enabled for optimal GPU performance\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Mixed precision setup failed: {e}\")\n",
    "else:\n",
    "    print(\"üíª CPU-only mode - optimizing for CPU performance\")\n",
    "    # CPU optimizations\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(0)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(0)\n",
    "    print(\"‚úÖ CPU threading optimized for maximum performance\")\n",
    "\n",
    "print(\"\\n=== Environment Status ===\")\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"üéâ GPU setup complete!\")\n",
    "    print(\"üìà Training will use GPU acceleration\")\n",
    "else:\n",
    "    print(\"‚úÖ CPU setup complete!\")\n",
    "    print(\"üìä Training will use optimized CPU performance\")\n",
    "    print(\"‚è±Ô∏è Expected training time: 2-3x longer than GPU (still manageable)\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7edbf4b",
   "metadata": {},
   "source": [
    "## If there exists GPU issue, please run the cell below to troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß GPU Troubleshooting and Setup Assistant\n",
    "# Run this cell if you're experiencing GPU issues\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_command(command, description):\n",
    "    \"\"\"Run a system command and return the result\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            return True, result.stdout.strip()\n",
    "        else:\n",
    "            return False, result.stderr.strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, \"Command timed out\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"üîç GPU Troubleshooting Assistant\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Check NVIDIA GPU\n",
    "print(\"\\n1Ô∏è‚É£ Checking NVIDIA GPU...\")\n",
    "success, output = run_command(\"nvidia-smi\", \"NVIDIA GPU check\")\n",
    "if success:\n",
    "    print(\"‚úÖ NVIDIA GPU detected:\")\n",
    "    # Extract GPU info from nvidia-smi output\n",
    "    lines = output.split('\\n')\n",
    "    for line in lines:\n",
    "        if 'RTX' in line or 'GTX' in line or 'Tesla' in line or 'GeForce' in line:\n",
    "            print(f\"   üéÆ {line.strip()}\")\n",
    "else:\n",
    "    print(\"‚ùå NVIDIA GPU not detected or nvidia-smi not available\")\n",
    "    print(\"üí° Possible solutions:\")\n",
    "    print(\"   - Install NVIDIA drivers: sudo apt update && sudo apt install nvidia-driver-xxx\")\n",
    "    print(\"   - Check if GPU is properly connected\")\n",
    "    print(\"   - Restart your system after driver installation\")\n",
    "\n",
    "# 2. Check CUDA installation\n",
    "print(\"\\n2Ô∏è‚É£ Checking CUDA installation...\")\n",
    "success, output = run_command(\"nvcc --version\", \"CUDA compiler check\")\n",
    "if success:\n",
    "    print(\"‚úÖ CUDA installed:\")\n",
    "    for line in output.split('\\n'):\n",
    "        if 'release' in line.lower():\n",
    "            print(f\"   üì¶ {line.strip()}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not installed or not in PATH\")\n",
    "    print(\"üí° Install CUDA:\")\n",
    "    print(\"   Ubuntu 24.04: https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=24.04&target_type=deb_network\")\n",
    "\n",
    "# 3. Check CUDA environment variables\n",
    "print(\"\\n3Ô∏è‚É£ Checking CUDA environment...\")\n",
    "cuda_home = os.environ.get('CUDA_HOME', os.environ.get('CUDA_PATH', 'Not set'))\n",
    "ld_library_path = os.environ.get('LD_LIBRARY_PATH', 'Not set')\n",
    "\n",
    "print(f\"   CUDA_HOME: {cuda_home}\")\n",
    "if 'cuda' not in cuda_home.lower():\n",
    "    print(\"   ‚ö†Ô∏è CUDA_HOME not properly set\")\n",
    "    print(\"   üí° Add to ~/.bashrc: export CUDA_HOME=/usr/local/cuda\")\n",
    "\n",
    "# 4. Check TensorFlow GPU support\n",
    "print(\"\\n4Ô∏è‚É£ Checking TensorFlow GPU support...\")\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Suppress warnings for this check\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    \n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"‚úÖ TensorFlow can see {len(gpus)} GPU(s)\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"   üéÆ GPU {i}: {gpu}\")\n",
    "        \n",
    "        # Test GPU computation\n",
    "        try:\n",
    "            with tf.device('/GPU:0'):\n",
    "                a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "                b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "                c = tf.matmul(a, b)\n",
    "            print(\"‚úÖ GPU computation test successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå GPU computation test failed: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå TensorFlow cannot detect GPU\")\n",
    "        print(\"üí° Possible solutions:\")\n",
    "        print(\"   - Reinstall TensorFlow with GPU support: pip install tensorflow[and-cuda]\")\n",
    "        print(\"   - Check CUDA compatibility: https://www.tensorflow.org/install/source#gpu\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå TensorFlow import failed: {e}\")\n",
    "\n",
    "# 5. Provide installation commands\n",
    "print(\"\\n5Ô∏è‚É£ Quick Fix Commands (if needed):\")\n",
    "print(\"   # Install NVIDIA drivers\")\n",
    "print(\"   sudo apt update && sudo apt install nvidia-driver-535\")\n",
    "print(\"   \")\n",
    "print(\"   # Install CUDA (Ubuntu 24.04)\")\n",
    "print(\"   wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb\")\n",
    "print(\"   sudo dpkg -i cuda-keyring_1.1-1_all.deb\")\n",
    "print(\"   sudo apt-get update\")\n",
    "print(\"   sudo apt-get -y install cuda-toolkit-12-4\")\n",
    "print(\"   \")\n",
    "print(\"   # Add to ~/.bashrc\")\n",
    "print(\"   echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc\")\n",
    "print(\"   echo 'export PATH=$CUDA_HOME/bin:$PATH' >> ~/.bashrc\")\n",
    "print(\"   echo 'export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\")\n",
    "print(\"   source ~/.bashrc\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"üîÑ After installing CUDA, restart your kernel: Kernel ‚Üí Restart Kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae5deda",
   "metadata": {},
   "source": [
    "# üéØ **Quick Start Guide - TensorFlow 2.19.0**\n",
    "\n",
    "## Getting Started (Cross-Platform)\n",
    "\n",
    "### 0Ô∏è‚É£ **GPU Setup (Optional but Recommended)**\n",
    "For optimal performance, install CUDA and cuDNN first:\n",
    "- **Ubuntu 24.04**: https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=24.04&target_type=deb_network\n",
    "- **Other OS**: https://developer.nvidia.com/cuda-downloads\n",
    "- Follow NVIDIA's installation instructions for your specific OS\n",
    "\n",
    "### 1Ô∏è‚É£ **Install Dependencies**\n",
    "Run the installation cells above:\n",
    "- ‚úÖ **Auto-detects** your OS and Python version\n",
    "- ‚úÖ **Downloads correct** TensorFlow wheel for your system\n",
    "- ‚úÖ **Installs all** required dependencies\n",
    "\n",
    "### 2Ô∏è‚É£ **Restart Kernel**  \n",
    "Go to **Kernel ‚Üí Restart Kernel** in VS Code\n",
    "\n",
    "### 3Ô∏è‚É£ **Verify Setup**\n",
    "Re-run the imports cell to verify everything works\n",
    "\n",
    "### 4Ô∏è‚É£ **Start Training**\n",
    "Continue with the car classification cells below!\n",
    "\n",
    "---\n",
    "\n",
    "## üñ•Ô∏è **Platform-Specific Notes**\n",
    "\n",
    "### üêß **Linux/WSL2 (Recommended)**\n",
    "- **GPU Support**: Full CUDA acceleration available with proper setup\n",
    "- **Best Performance**: Ideal for intensive training workloads\n",
    "- **CUDA Guide**: Follow the Ubuntu 24.04 link above for your distribution\n",
    "\n",
    "### ü™ü **Windows**\n",
    "- **GPU Support**: Use WSL2 for full GPU acceleration\n",
    "- **CPU-only**: Still excellent performance with multi-core optimization\n",
    "- **Training time**: ~2-3x longer than GPU, but manageable for this project\n",
    "- **Memory**: 16GB+ RAM recommended for larger batch sizes\n",
    "\n",
    "### üçé **macOS**\n",
    "- **Apple Silicon**: Optimized for M1/M2/M3 chips\n",
    "- **Intel**: CPU-only, still very capable\n",
    "\n",
    "**TensorFlow 2.19 brings improved CPU performance across all platforms!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ab38e",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Preprocessing\n",
    "\n",
    "## Objective\n",
    "Prepare the Stanford Cars Dataset for training by downloading, preprocessing, and splitting it into appropriate sets.\n",
    "\n",
    "**Thought Process: The Stanford Cars Dataset has 196 classes, requiring robust preprocessing. We'll download via KaggleHub for reliability, resize to 224x224 for ResNet50, normalize pixels for consistency, and use augmentation to handle variations. Splitting into train/validation/test sets, with 20% validation from training, ensures proper evaluation.**\n",
    "\n",
    "## Dataset Details\n",
    "Source: The Stanford Cars Dataset contains 16,185 images across 196 classes (Make, Model, Year), with 8,144 training and 8,041 testing images. Images are approximately 360x240 pixels.\n",
    "\n",
    "Download: Access the dataset from Kaggle or the official site (note: the official URL seems to be broken, so I'll download from Kaggle).\n",
    "\n",
    "Structure: The dataset includes images and annotations in .mat format, requiring scipy to load.\n",
    "\n",
    "## Preprocessing Steps\n",
    "Resize Images: Resize images to 224x224 pixels to match the input size of pre-trained models like ResNet50.\n",
    "\n",
    "Normalization: Scale pixel values to [0, 1] by dividing by 255.\n",
    "\n",
    "Augmentation: Apply random flips, rotations, and color jitter to improve model generalization.\n",
    "\n",
    "Data Splitting: Use the provided train/test split. Create a validation set by taking 20% of the training data (approximately 1,629 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "download_path = kagglehub.dataset_download(\"cyizhuo/stanford-cars-by-classes-folder\")\n",
    "\n",
    "print(\"Path to dataset files:\", download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Robust Dataset Structure Detection and Organization\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_dataset_structure(downloaded_path):\n",
    "    \"\"\"\n",
    "    Robustly detect and analyze the Stanford Cars dataset folder structure\n",
    "    Handles multiple possible formats from different sources\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîç DETECTING DATASET STRUCTURE...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check all possible locations where dataset might be\n",
    "    possible_paths = [\n",
    "        Path(downloaded_path),\n",
    "        Path(\"data\"),\n",
    "        Path(\".\"),\n",
    "        Path(\"stanford-cars-by-classes-folder\"),\n",
    "        Path(\"~/.cache/kagglehub\").expanduser(),\n",
    "    ]\n",
    "    \n",
    "    dataset_info = {\n",
    "        'found': False,\n",
    "        'location': None,\n",
    "        'structure_type': None,\n",
    "        'train_path': None,\n",
    "        'test_path': None,\n",
    "        'classes': [],\n",
    "        'total_images': 0\n",
    "    }\n",
    "    \n",
    "    for base_path in possible_paths:\n",
    "        if not base_path.exists():\n",
    "            continue\n",
    "            \n",
    "        print(f\"üîç Checking: {base_path.absolute()}\")\n",
    "        \n",
    "        # Look for different possible structures\n",
    "        structures_to_check = [\n",
    "            # Structure 1: Direct train/test folders\n",
    "            {'train': base_path / 'train', 'test': base_path / 'test', 'type': 'direct_train_test'},\n",
    "            {'train': base_path / 'cars_train', 'test': base_path / 'cars_test', 'type': 'cars_prefix'},\n",
    "            \n",
    "            # Structure 2: Nested in subdirectories\n",
    "            {'train': base_path / 'stanford-cars-dataset' / 'train', 'test': base_path / 'stanford-cars-dataset' / 'test', 'type': 'nested_stanford'},\n",
    "            {'train': base_path / 'stanford-cars-by-classes-folder' / 'train', 'test': base_path / 'stanford-cars-by-classes-folder' / 'test', 'type': 'nested_classes'},\n",
    "            \n",
    "            # Structure 3: Single folder with all classes (need to split)\n",
    "            {'train': base_path / 'all_classes', 'test': None, 'type': 'single_folder'},\n",
    "            \n",
    "            # Structure 4: Kagglehub cache structure\n",
    "            {'train': None, 'test': None, 'type': 'kagglehub_cache'},\n",
    "        ]\n",
    "        \n",
    "        for structure in structures_to_check:\n",
    "            if structure['type'] == 'kagglehub_cache':\n",
    "                # Special handling for kagglehub cache\n",
    "                if 'kagglehub' in str(base_path):\n",
    "                    for item in base_path.rglob('*'):\n",
    "                        if item.is_dir() and ('train' in item.name.lower() or 'test' in item.name.lower()):\n",
    "                            print(f\"   üìÅ Found kagglehub folder: {item}\")\n",
    "                            dataset_info['found'] = True\n",
    "                            dataset_info['location'] = base_path\n",
    "                            dataset_info['structure_type'] = 'kagglehub_cache'\n",
    "                            break\n",
    "                continue\n",
    "            \n",
    "            train_path = structure['train']\n",
    "            test_path = structure['test']\n",
    "            \n",
    "            # Check if this structure exists\n",
    "            train_exists = train_path and train_path.exists() and train_path.is_dir()\n",
    "            test_exists = test_path and test_path.exists() and test_path.is_dir()\n",
    "            \n",
    "            if train_exists or test_exists:\n",
    "                print(f\"   ‚úÖ Found structure type: {structure['type']}\")\n",
    "                print(f\"   üìÅ Train folder: {train_path} ({'‚úÖ' if train_exists else '‚ùå'})\")\n",
    "                print(f\"   üìÅ Test folder: {test_path} ({'‚úÖ' if test_exists else '‚ùå'})\")\n",
    "                \n",
    "                # Analyze the structure\n",
    "                classes = []\n",
    "                total_images = 0\n",
    "                \n",
    "                if train_exists:\n",
    "                    classes = [d.name for d in train_path.iterdir() if d.is_dir()]\n",
    "                    for class_dir in train_path.iterdir():\n",
    "                        if class_dir.is_dir():\n",
    "                            image_count = len([f for f in class_dir.iterdir() if f.suffix.lower() in ['.jpg', '.jpeg', '.png']])\n",
    "                            total_images += image_count\n",
    "                \n",
    "                dataset_info.update({\n",
    "                    'found': True,\n",
    "                    'location': base_path,\n",
    "                    'structure_type': structure['type'],\n",
    "                    'train_path': train_path if train_exists else None,\n",
    "                    'test_path': test_path if test_exists else None,\n",
    "                    'classes': classes[:10],  # Show first 10 classes\n",
    "                    'total_images': total_images\n",
    "                })\n",
    "                \n",
    "                print(f\"   üìä Classes found: {len(classes)} (showing first 10)\")\n",
    "                for i, class_name in enumerate(classes[:10]):\n",
    "                    print(f\"      {i+1:2d}. {class_name}\")\n",
    "                if len(classes) > 10:\n",
    "                    print(f\"      ... and {len(classes) - 10} more classes\")\n",
    "                print(f\"   üñºÔ∏è Total training images: {total_images}\")\n",
    "                \n",
    "                return dataset_info\n",
    "    \n",
    "    print(\"‚ùå No Stanford Cars dataset found in any expected location\")\n",
    "    return dataset_info\n",
    "\n",
    "def organize_dataset_structure(dataset_info):\n",
    "    \"\"\"\n",
    "    Organize the dataset into a standard structure for training\n",
    "    \"\"\"\n",
    "    \n",
    "    if not dataset_info['found']:\n",
    "        print(\"‚ùå Cannot organize dataset - no dataset found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüîß ORGANIZING DATASET STRUCTURE...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create standard data directory structure\n",
    "    standard_data_dir = Path(\"data\")\n",
    "    standard_train_dir = standard_data_dir / \"train\"\n",
    "    standard_test_dir = standard_data_dir / \"test\"\n",
    "    \n",
    "    standard_data_dir.mkdir(exist_ok=True)\n",
    "    standard_train_dir.mkdir(exist_ok=True)\n",
    "    standard_test_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    structure_type = dataset_info['structure_type']\n",
    "    \n",
    "    if structure_type in ['direct_train_test', 'cars_prefix', 'nested_stanford', 'nested_classes']:\n",
    "        # Copy/link existing train and test folders\n",
    "        \n",
    "        if dataset_info['train_path'] and dataset_info['train_path'].exists():\n",
    "            print(f\"üìÅ Organizing training data from: {dataset_info['train_path']}\")\n",
    "            \n",
    "            if not any(standard_train_dir.iterdir()):  # Only copy if empty\n",
    "                for class_dir in dataset_info['train_path'].iterdir():\n",
    "                    if class_dir.is_dir():\n",
    "                        dest_class_dir = standard_train_dir / class_dir.name\n",
    "                        if not dest_class_dir.exists():\n",
    "                            print(f\"   üìÇ Copying class: {class_dir.name}\")\n",
    "                            shutil.copytree(class_dir, dest_class_dir)\n",
    "            else:\n",
    "                print(\"   ‚úÖ Training data already organized\")\n",
    "        \n",
    "        if dataset_info['test_path'] and dataset_info['test_path'].exists():\n",
    "            print(f\"üìÅ Organizing test data from: {dataset_info['test_path']}\")\n",
    "            \n",
    "            if not any(standard_test_dir.iterdir()):  # Only copy if empty\n",
    "                for class_dir in dataset_info['test_path'].iterdir():\n",
    "                    if class_dir.is_dir():\n",
    "                        dest_class_dir = standard_test_dir / class_dir.name\n",
    "                        if not dest_class_dir.exists():\n",
    "                            print(f\"   üìÇ Copying class: {class_dir.name}\")\n",
    "                            shutil.copytree(class_dir, dest_class_dir)\n",
    "            else:\n",
    "                print(\"   ‚úÖ Test data already organized\")\n",
    "    \n",
    "    elif structure_type == 'single_folder':\n",
    "        # Split single folder into train/test (80/20 split)\n",
    "        print(\"üìÅ Splitting single folder into train/test sets...\")\n",
    "        \n",
    "        source_path = dataset_info['location']\n",
    "        # Implementation for splitting would go here\n",
    "        print(\"‚ö†Ô∏è Single folder splitting not yet implemented - please organize manually\")\n",
    "    \n",
    "    elif structure_type == 'kagglehub_cache':\n",
    "        print(\"üìÅ Detecting kagglehub cache structure...\")\n",
    "        # Find the actual dataset files in kagglehub cache\n",
    "        cache_path = dataset_info['location']\n",
    "        \n",
    "        # Look for train/test folders in cache\n",
    "        for item in cache_path.rglob('*'):\n",
    "            if item.is_dir() and 'train' in item.name.lower():\n",
    "                print(f\"   üìÇ Found train folder: {item}\")\n",
    "                dataset_info['train_path'] = item\n",
    "            elif item.is_dir() and 'test' in item.name.lower():\n",
    "                print(f\"   üìÇ Found test folder: {item}\")\n",
    "                dataset_info['test_path'] = item\n",
    "        \n",
    "        # Recursive call with updated paths\n",
    "        return organize_dataset_structure(dataset_info)\n",
    "    \n",
    "    # Verify final structure\n",
    "    train_classes = [d.name for d in standard_train_dir.iterdir() if d.is_dir()]\n",
    "    test_classes = [d.name for d in standard_test_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(f\"\\n‚úÖ DATASET ORGANIZATION COMPLETE!\")\n",
    "    print(f\"üìä Training classes: {len(train_classes)}\")\n",
    "    print(f\"üìä Test classes: {len(test_classes)}\")\n",
    "    \n",
    "    # Count images\n",
    "    total_train_images = sum(len([f for f in class_dir.iterdir() if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]) \n",
    "                           for class_dir in standard_train_dir.iterdir() if class_dir.is_dir())\n",
    "    total_test_images = sum(len([f for f in class_dir.iterdir() if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]) \n",
    "                          for class_dir in standard_test_dir.iterdir() if class_dir.is_dir())\n",
    "    \n",
    "    print(f\"üñºÔ∏è Training images: {total_train_images}\")\n",
    "    print(f\"üñºÔ∏è Test images: {total_test_images}\")\n",
    "    \n",
    "    return {\n",
    "        'train_dir': standard_train_dir,\n",
    "        'test_dir': standard_test_dir,\n",
    "        'train_classes': train_classes,\n",
    "        'test_classes': test_classes,\n",
    "        'train_images': total_train_images,\n",
    "        'test_images': total_test_images\n",
    "    }\n",
    "\n",
    "def verify_dataset_compatibility():\n",
    "    \"\"\"\n",
    "    Verify that the organized dataset is compatible with Keras ImageDataGenerator\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç VERIFYING DATASET COMPATIBILITY...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    data_dir = Path(\"data\")\n",
    "    train_dir = data_dir / \"train\"\n",
    "    test_dir = data_dir / \"test\"\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check basic structure\n",
    "    if not train_dir.exists():\n",
    "        issues.append(\"‚ùå Train directory missing\")\n",
    "    else:\n",
    "        print(\"‚úÖ Train directory found\")\n",
    "    \n",
    "    if not test_dir.exists():\n",
    "        issues.append(\"‚ö†Ô∏è Test directory missing (optional)\")\n",
    "    else:\n",
    "        print(\"‚úÖ Test directory found\")\n",
    "    \n",
    "    # Check class structure\n",
    "    if train_dir.exists():\n",
    "        train_classes = [d for d in train_dir.iterdir() if d.is_dir()]\n",
    "        if len(train_classes) == 0:\n",
    "            issues.append(\"‚ùå No class folders found in train directory\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Found {len(train_classes)} training classes\")\n",
    "            \n",
    "            # Check if classes have images\n",
    "            empty_classes = []\n",
    "            for class_dir in train_classes:\n",
    "                image_files = [f for f in class_dir.iterdir() if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "                if len(image_files) == 0:\n",
    "                    empty_classes.append(class_dir.name)\n",
    "            \n",
    "            if empty_classes:\n",
    "                issues.append(f\"‚ö†Ô∏è Empty classes found: {', '.join(empty_classes[:5])}\")\n",
    "            else:\n",
    "                print(\"‚úÖ All classes contain images\")\n",
    "    \n",
    "    # Check file formats\n",
    "    if train_dir.exists():\n",
    "        all_files = list(train_dir.rglob('*'))\n",
    "        image_files = [f for f in all_files if f.is_file() and f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "        other_files = [f for f in all_files if f.is_file() and f.suffix.lower() not in ['.jpg', '.jpeg', '.png']]\n",
    "        \n",
    "        print(f\"üìä Image files: {len(image_files)}\")\n",
    "        if other_files:\n",
    "            print(f\"‚ö†Ô∏è Non-image files: {len(other_files)} (first 5: {[f.name for f in other_files[:5]]})\")\n",
    "    \n",
    "    # Summary\n",
    "    if issues:\n",
    "        print(f\"\\n‚ö†Ô∏è ISSUES FOUND:\")\n",
    "        for issue in issues:\n",
    "            print(f\"   {issue}\")\n",
    "        print(\"\\nüí° You may need to manually fix these issues before training\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ DATASET READY FOR TRAINING!\")\n",
    "        print(\"üöÄ All checks passed - dataset is compatible with ImageDataGenerator\")\n",
    "    \n",
    "    return len(issues) == 0\n",
    "\n",
    "# üöÄ Execute Dataset Detection and Organization\n",
    "\n",
    "print(\"üéØ STANFORD CARS DATASET DETECTION AND ORGANIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Detect dataset structure\n",
    "dataset_info = detect_dataset_structure(download_path)\n",
    "\n",
    "if dataset_info['found']:\n",
    "    print(f\"\\nüéâ Dataset found!\")\n",
    "    print(f\"üìç Location: {dataset_info['location']}\")\n",
    "    print(f\"üèóÔ∏è Structure type: {dataset_info['structure_type']}\")\n",
    "    \n",
    "    # Step 2: Organize dataset\n",
    "    organized_info = organize_dataset_structure(dataset_info)\n",
    "    \n",
    "    if organized_info:\n",
    "        # Step 3: Verify compatibility\n",
    "        is_compatible = verify_dataset_compatibility()\n",
    "        \n",
    "        if is_compatible:\n",
    "            print(f\"\\nüéØ DATASET SETUP SUCCESSFUL!\")\n",
    "            print(\"‚úÖ Ready to proceed with model training\")\n",
    "            \n",
    "            # Store for later use\n",
    "            DATASET_READY = True\n",
    "            TRAIN_DIR = organized_info['train_dir']\n",
    "            TEST_DIR = organized_info['test_dir']\n",
    "            NUM_CLASSES = len(organized_info['train_classes'])\n",
    "            \n",
    "            print(f\"üìä Configuration:\")\n",
    "            print(f\"   Train directory: {TRAIN_DIR}\")\n",
    "            print(f\"   Test directory: {TEST_DIR}\")\n",
    "            print(f\"   Number of classes: {NUM_CLASSES}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Dataset has issues - may need manual fixing\")\n",
    "            DATASET_READY = False\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Failed to organize dataset\")\n",
    "        DATASET_READY = False\n",
    "else:\n",
    "    print(f\"\\nüì• No dataset found. Options:\")\n",
    "    print(\"1. Run the previous cell to download via kagglehub\")\n",
    "    print(\"2. Download manually and place in 'data' folder\")\n",
    "    print(\"3. Continue with demo data for testing\")\n",
    "    DATASET_READY = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46afc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stanford_cars_metadata():\n",
    "    \"\"\"\n",
    "    Load Stanford Cars dataset metadata and class information\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä Loading Stanford Cars Dataset Metadata...\")\n",
    "    \n",
    "    # Stanford Cars class names (196 classes)\n",
    "    car_classes = [\n",
    "        \"AM General Hummer SUV 2000\", \"Acura RL Sedan 2012\", \"Acura TL Sedan 2012\", \n",
    "        \"Acura TL Type-S 2008\", \"Acura TSX Sedan 2012\", \"Acura Integra Type R 2001\",\n",
    "        \"Acura ZDX Hatchback 2012\", \"Aston Martin V8 Vantage Convertible 2012\",\n",
    "        \"Aston Martin V8 Vantage Coupe 2012\", \"Aston Martin Virage Convertible 2012\",\n",
    "        \"Aston Martin Virage Coupe 2012\", \"Audi RS 4 Convertible 2008\",\n",
    "        \"Audi A5 Coupe 2012\", \"Audi TTS Coupe 2012\", \"Audi R8 Coupe 2012\",\n",
    "        \"Audi V8 Sedan 1994\", \"Audi 100 Sedan 1994\", \"Audi 100 Wagon 1994\",\n",
    "        \"Audi TT Hatchback 2011\", \"Audi S6 Sedan 2011\", \"Audi S5 Convertible 2012\",\n",
    "        \"Audi S5 Coupe 2012\", \"Audi S4 Sedan 2007\", \"Audi S4 Sedan 2012\",\n",
    "        \"Audi TT RS Coupe 2012\", \"BMW ActiveHybrid 5 Sedan 2012\", \"BMW 1 Series Convertible 2012\",\n",
    "        \"BMW 1 Series Coupe 2012\", \"BMW 3 Series Sedan 2012\", \"BMW 3 Series Wagon 2012\",\n",
    "        \"BMW 6 Series Convertible 2007\", \"BMW X5 SUV 2007\", \"BMW X6 SUV 2012\",\n",
    "        \"BMW M3 Coupe 2012\", \"BMW M5 Sedan 2010\", \"BMW M6 Convertible 2010\",\n",
    "        \"BMW X3 SUV 2012\", \"BMW Z4 Convertible 2012\", \"Bentley Continental Supersports Conv. Convertible 2012\",\n",
    "        \"Bentley Arnage Sedan 2009\", \"Bentley Mulsanne Sedan 2011\", \"Bentley Continental GT Coupe 2007\",\n",
    "        \"Bentley Continental GT Coupe 2012\", \"Bentley Continental Flying Spur Sedan 2007\",\n",
    "        \"Bugatti Veyron 16.4 Convertible 2009\", \"Bugatti Veyron 16.4 Coupe 2009\",\n",
    "        \"Buick Regal GS 2012\", \"Buick Rainier SUV 2007\", \"Buick Verano Sedan 2012\",\n",
    "        \"Buick Enclave SUV 2012\", \"Cadillac CTS-V Sedan 2011\", \"Cadillac SRX SUV 2012\",\n",
    "        \"Cadillac Escalade EXT Crew Cab 2007\", \"Chevrolet Silverado 1500 Hybrid Crew Cab 2012\",\n",
    "        \"Chevrolet Corvette Convertible 2012\", \"Chevrolet Corvette ZR1 2012\",\n",
    "        \"Chevrolet Corvette Ron Fellows Edition Z06 2007\", \"Chevrolet Traverse SUV 2012\",\n",
    "        \"Chevrolet Camaro Convertible 2012\", \"Chevrolet HHR SS 2010\", \"Chevrolet Impala Sedan 2007\",\n",
    "        \"Chevrolet Tahoe Hybrid SUV 2012\", \"Chevrolet Sonic Sedan 2012\", \"Chevrolet Express Cargo Van 2007\",\n",
    "        \"Chevrolet Avalanche Crew Cab 2012\", \"Chevrolet Cobalt SS 2010\", \"Chevrolet Malibu Hybrid Sedan 2010\",\n",
    "        \"Chevrolet TrailBlazer SS 2009\", \"Chevrolet Silverado 2500HD Regular Cab 2012\",\n",
    "        \"Chevrolet Silverado 1500 Classic Extended Cab 2007\", \"Chevrolet Express Van 2007\",\n",
    "        \"Chevrolet Monte Carlo Coupe 2007\", \"Chevrolet Malibu Sedan 2007\", \"Chevrolet Silverado 1500 Extended Cab 2012\",\n",
    "        \"Chevrolet Silverado 1500 Regular Cab 2012\", \"Chrysler Aspen SUV 2009\", \"Chrysler Sebring Convertible 2010\",\n",
    "        \"Chrysler Town and Country Minivan 2012\", \"Daewoo Nubira Wagon 2002\", \"Dodge Caliber Wagon 2012\",\n",
    "        \"Dodge Caliber Wagon 2007\", \"Dodge Caravan Minivan 1997\", \"Dodge Ram Pickup 3500 Crew Cab 2010\",\n",
    "        \"Dodge Ram Pickup 3500 Quad Cab 2009\", \"Dodge Sprinter Cargo Van 2009\", \"Dodge Journey SUV 2012\",\n",
    "        \"Dodge Dakota Crew Cab 2010\", \"Dodge Dakota Club Cab 2007\", \"Dodge Magnum Wagon 2008\",\n",
    "        \"Dodge Challenger SRT8 2011\", \"Dodge Durango SUV 2012\", \"Dodge Durango SUV 2007\",\n",
    "        \"Dodge Charger Sedan 2012\", \"Dodge Charger SRT-8 2009\", \"Eagle Talon Hatchback 1998\",\n",
    "        \"FIAT 500 Abarth 2012\", \"FIAT 500 Convertible 2012\", \"Ferrari FF Coupe 2012\",\n",
    "        \"Ferrari California Convertible 2012\", \"Ferrari 458 Italia Convertible 2012\", \"Ferrari 458 Italia Coupe 2012\",\n",
    "        \"Fisker Karma Sedan 2012\", \"Ford F-450 Super Duty Crew Cab 2012\", \"Ford Mustang Convertible 2007\",\n",
    "        \"Ford Freestar Minivan 2007\", \"Ford Expedition EL SUV 2009\", \"Ford Edge SUV 2012\",\n",
    "        \"Ford Ranger SuperCab 2011\", \"Ford GT Coupe 2006\", \"Ford F-150 Regular Cab 2012\",\n",
    "        \"Ford F-150 Regular Cab 2007\", \"Ford Focus Sedan 2007\", \"Ford E-Series Wagon Van 2012\",\n",
    "        \"Ford Fiesta Sedan 2012\", \"GMC Terrain SUV 2012\", \"GMC Savana Van 2012\",\n",
    "        \"GMC Yukon Hybrid SUV 2012\", \"GMC Acadia SUV 2012\", \"GMC Canyon Extended Cab 2012\",\n",
    "        \"Geo Metro Convertible 1993\", \"HUMMER H3T Crew Cab 2010\", \"HUMMER H2 SUT Crew Cab 2009\",\n",
    "        \"Honda Odyssey Minivan 2012\", \"Honda Odyssey Minivan 2007\", \"Honda Accord Coupe 2012\",\n",
    "        \"Honda Accord Sedan 2012\", \"Hyundai Veloster Hatchback 2012\", \"Hyundai Santa Fe SUV 2012\",\n",
    "        \"Hyundai Tucson SUV 2012\", \"Hyundai Veracruz SUV 2012\", \"Hyundai Sonata Hybrid Sedan 2012\",\n",
    "        \"Hyundai Elantra Sedan 2007\", \"Hyundai Accent Sedan 2012\", \"Hyundai Elantra Touring Hatchback 2012\",\n",
    "        \"Hyundai Sonata Sedan 2012\", \"Infiniti G Coupe IPL 2012\", \"Infiniti QX56 SUV 2011\",\n",
    "        \"Isuzu Ascender SUV 2008\", \"Jaguar XK XKR 2012\", \"Jeep Patriot SUV 2012\",\n",
    "        \"Jeep Wrangler SUV 2012\", \"Jeep Liberty SUV 2012\", \"Jeep Grand Cherokee SUV 2012\",\n",
    "        \"Jeep Compass SUV 2012\", \"Lamborghini Reventon Coupe 2008\", \"Lamborghini Aventador Coupe 2012\",\n",
    "        \"Lamborghini Gallardo LP 570-4 Superleggera 2012\", \"Lamborghini Diablo Coupe 2001\",\n",
    "        \"Land Rover Range Rover SUV 2012\", \"Land Rover LR2 SUV 2012\", \"Lincoln Town Car Sedan 2011\",\n",
    "        \"MINI Cooper Roadster Convertible 2012\", \"Maybach Landaulet Convertible 2012\", \"Mazda Tribute SUV 2011\",\n",
    "        \"McLaren MP4-12C Coupe 2012\", \"Mercedes-Benz 300-Class Convertible 1993\", \"Mercedes-Benz C-Class Sedan 2012\",\n",
    "        \"Mercedes-Benz SL-Class Coupe 2009\", \"Mercedes-Benz E-Class Sedan 2012\", \"Mercedes-Benz S-Class Sedan 2012\",\n",
    "        \"Mercedes-Benz Sprinter Van 2012\", \"Mitsubishi Lancer Evolution 2004\", \"Nissan Leaf Hatchback 2012\",\n",
    "        \"Nissan NV200 Minivan 2013\", \"Nissan Juke Hatchback 2012\", \"Nissan 240SX Coupe 1998\",\n",
    "        \"Plymouth Neon Coupe 1999\", \"Porsche Panamera Sedan 2012\", \"Ram C/V Cargo Van Minivan 2012\",\n",
    "        \"Rolls-Royce Phantom Drophead Coupe Convertible 2012\", \"Rolls-Royce Ghost Sedan 2012\",\n",
    "        \"Rolls-Royce Phantom Sedan 2012\", \"Scion xD Hatchback 2012\", \"Spyker C8 Convertible 2009\",\n",
    "        \"Spyker C8 Coupe 2009\", \"Suzuki Aerio Sedan 2007\", \"Suzuki Kizashi Sedan 2012\",\n",
    "        \"Suzuki SX4 Hatchback 2012\", \"Suzuki SX4 Sedan 2012\", \"Tesla Model S Sedan 2012\",\n",
    "        \"Toyota Sequoia SUV 2012\", \"Toyota Camry Sedan 2012\", \"Toyota Corolla Sedan 2012\",\n",
    "        \"Toyota 4Runner SUV 2012\", \"Volkswagen Golf Hatchback 2012\", \"Volkswagen Golf Hatchback 1991\",\n",
    "        \"Volkswagen Beetle Hatchback 2012\", \"Volvo C30 Hatchbook 2012\", \"Volvo 240 Sedan 1993\",\n",
    "        \"Volvo XC90 SUV 2007\", \"smart fortwo Convertible 2012\"\n",
    "    ]\n",
    "    \n",
    "    # Create class mappings\n",
    "    class_to_index = {cls: idx for idx, cls in enumerate(car_classes)}\n",
    "    index_to_class = {idx: cls for idx, cls in enumerate(car_classes)}\n",
    "    \n",
    "    return car_classes, class_to_index, index_to_class\n",
    "\n",
    "# Utilize detected dataset paths and setup metadata\n",
    "print(f\"\\nüîß UTILIZING DETECTED DATASET STRUCTURE...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if dataset was properly detected and organized\n",
    "if 'DATASET_READY' in globals() and DATASET_READY:\n",
    "    print(f\"‚úÖ Dataset ready for use!\")\n",
    "    print(f\"üìÅ Train directory: {TRAIN_DIR}\")\n",
    "    print(f\"üìÅ Test directory: {TEST_DIR}\")\n",
    "    \n",
    "    # Use the detected paths\n",
    "    train_dir = TRAIN_DIR\n",
    "    test_dir = TEST_DIR\n",
    "    \n",
    "    # Get actual class names from the organized dataset\n",
    "    actual_classes = [d.name for d in train_dir.iterdir() if d.is_dir()]\n",
    "    actual_classes.sort()  # Sort for consistency\n",
    "    \n",
    "    print(f\"üìä Detected {len(actual_classes)} classes from dataset structure\")\n",
    "    print(f\"üè∑Ô∏è Sample classes: {actual_classes[:5]}\")\n",
    "    \n",
    "    # Update NUM_CLASSES to match actual dataset\n",
    "    NUM_CLASSES = len(actual_classes)\n",
    "    \n",
    "    # Create class mappings from actual dataset structure\n",
    "    class_to_index = {cls: idx for idx, cls in enumerate(actual_classes)}\n",
    "    index_to_class = {idx: cls for idx, cls in enumerate(actual_classes)}\n",
    "    car_classes = actual_classes\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Dataset not properly detected, using fallback metadata...\")\n",
    "    \n",
    "    # Fallback to predefined Stanford Cars class list\n",
    "    car_classes, class_to_index, index_to_class = load_stanford_cars_metadata()\n",
    "    NUM_CLASSES = len(car_classes)  # 196 classes for Stanford Cars\n",
    "    \n",
    "    # Set fallback paths\n",
    "    train_dir = Path(\"data/train\")\n",
    "    test_dir = Path(\"data/test\")\n",
    "    \n",
    "    print(f\"üìä Using predefined {NUM_CLASSES} Stanford Cars classes\")\n",
    "    print(f\"üè∑Ô∏è Sample classes: {car_classes[:5]}\")\n",
    "\n",
    "# Dataset configuration\n",
    "IMG_SIZE = (224, 224)  # Standard input size for ResNet50\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20  # Training epochs\n",
    "\n",
    "print(f\"\\nüìà FINAL DATASET CONFIGURATION:\")\n",
    "print(f\"   Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"   Image size: {IMG_SIZE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Training epochs: {EPOCHS}\")\n",
    "print(f\"   Train directory: {train_dir}\")\n",
    "print(f\"   Test directory: {test_dir}\")\n",
    "\n",
    "# Verify paths exist\n",
    "if train_dir.exists():\n",
    "    train_class_count = len([d for d in train_dir.iterdir() if d.is_dir()])\n",
    "    train_image_count = sum(len([f for f in class_dir.iterdir() if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]) \n",
    "                           for class_dir in train_dir.iterdir() if class_dir.is_dir())\n",
    "    print(f\"   ‚úÖ Train directory: {train_class_count} classes, {train_image_count} images\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Train directory not found: {train_dir}\")\n",
    "\n",
    "if test_dir.exists():\n",
    "    test_class_count = len([d for d in test_dir.iterdir() if d.is_dir()])\n",
    "    test_image_count = sum(len([f for f in class_dir.iterdir() if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]) \n",
    "                          for class_dir in test_dir.iterdir() if class_dir.is_dir())\n",
    "    print(f\"   ‚úÖ Test directory: {test_class_count} classes, {test_image_count} images\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Test directory not found: {test_dir}\")\n",
    "\n",
    "# Save class mapping for API and deployment use\n",
    "print(f\"\\nüíæ Saving class mapping...\")\n",
    "class_mapping = {\n",
    "    'class_to_index': class_to_index,\n",
    "    'index_to_class': index_to_class,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'class_names': car_classes\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('class_mapping.json', 'w') as f:\n",
    "    json.dump(class_mapping, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Class mapping saved to 'class_mapping.json'\")\n",
    "print(f\"üìã Mapping contains {len(class_mapping['class_names'])} classes\")\n",
    "\n",
    "# Display final status\n",
    "print(f\"\\nüéØ DATASET SETUP STATUS:\")\n",
    "if 'DATASET_READY' in globals() and DATASET_READY and train_dir.exists():\n",
    "    print(\"‚úÖ Dataset is ready for model training!\")\n",
    "    print(\"üöÄ You can proceed to the next cells for data preprocessing and model training\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset setup incomplete\")\n",
    "    print(\"üí° Please ensure dataset is properly downloaded and organized\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a046943",
   "metadata": {},
   "source": [
    "# 3. Model Architecture\n",
    "\n",
    "## Objective\n",
    "Build a deep learning model using transfer learning with a pre-trained backbone for car classification.\n",
    "\n",
    "**Thought Process: For 196 classes, transfer learning is efficient. ResNet50 is chosen for its proven performance on ImageNet, suitable for 224x224 inputs, balancing accuracy and computation. We'll freeze early layers initially, fine-tune later, assuming sufficient data for adaptation, and use dropout to prevent overfitting.**\n",
    "\n",
    "## Design Decisions\n",
    "**Backbone**: ResNet50 pre-trained on ImageNet\n",
    "- Proven architecture for image classification\n",
    "- Good balance between accuracy and computational efficiency\n",
    "- 224x224 input size standard\n",
    "\n",
    "**Transfer Learning Strategy**: \n",
    "- Freeze the convolutional base initially\n",
    "- Replace the top classification layer\n",
    "- Fine-tune the top layers after initial training\n",
    "\n",
    "**Architecture Assumptions**:\n",
    "- Input images: 224x224x3 (RGB)\n",
    "- Output: 196 classes (Stanford Cars Dataset)\n",
    "- Global Average Pooling to reduce parameters\n",
    "- Dropout for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "def create_car_classification_model():\n",
    "    \"\"\"\n",
    "    Create an improved CNN model for car classification using TF‚ÄØ2.19 best practices.\n",
    "    \"\"\"\n",
    "    print(\"üèóÔ∏è Building Enhanced Car Classification Model...\")\n",
    "\n",
    "    # 1) Base pretrained backbone\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(*IMG_SIZE, 3)\n",
    "    )\n",
    "\n",
    "    # 2) Freeze early layers, fine‚Äëtune last 20\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    print(f\"üìä Base model layers: {len(base_model.layers)}\")\n",
    "    print(f\"üîí Frozen layers: {len([l for l in base_model.layers if not l.trainable])}\")\n",
    "    print(f\"üîì Trainable layers: {len([l for l in base_model.layers if l.trainable])}\")\n",
    "\n",
    "    # 3) Build your head\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(\n",
    "            NUM_CLASSES,\n",
    "            activation='softmax',\n",
    "            kernel_regularizer=l2(0.01)\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # 4) Compile with modern metrics API\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Model compiled successfully!\")\n",
    "    print(\"\\nüìã Model Summary:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def setup_training_callbacks():\n",
    "    \"\"\"\n",
    "    Set up callbacks for TF‚ÄØ2.19 training:\n",
    "      - EarlyStopping\n",
    "      - ReduceLROnPlateau\n",
    "      - ModelCheckpoint in SavedModel/Keras format\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        ModelCheckpoint(\n",
    "            'best_car_model.keras',        # .keras = SavedModel/Keras v3 format\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    print(\"‚öôÔ∏è Training callbacks configured:\")\n",
    "    print(\"   ‚Ä¢ EarlyStopping (patience=10, restore_best_weights=True)\")\n",
    "    print(\"   ‚Ä¢ ReduceLROnPlateau (factor=0.2, patience=5)\")\n",
    "    print(\"   ‚Ä¢ ModelCheckpoint (best_car_model.keras, save_format='keras')\")\n",
    "\n",
    "    return callbacks\n",
    "\n",
    "# Usage\n",
    "model = create_car_classification_model()\n",
    "callbacks = setup_training_callbacks()\n",
    "\n",
    "print(f\"\\nüéØ Model ready for training on {NUM_CLASSES} car classes!\")\n",
    "print(f\"üìä Total parameters: {model.count_params():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34133373",
   "metadata": {},
   "source": [
    "# 4. Training Pipeline\n",
    "\n",
    "## Objective\n",
    "Train the model with transfer learning approach and monitor performance metrics.\n",
    "\n",
    "**Thought Process: We'll train in phases: first, freeze the backbone for quick adaptation (10-15 epochs), then fine-tune top layers (10-20 epochs) with lower learning rate. Batch size 32 balances memory and speed, Adam at 1e-4 is standard for fine-tuning, and callbacks prevent overfitting, ensuring robust training.**\n",
    "\n",
    "## Training Strategy\n",
    "1. **Phase 1**: Train with frozen backbone (feature extraction)\n",
    "   - Quick initial training to adapt the classifier head\n",
    "   - 10-15 epochs with higher learning rate\n",
    "\n",
    "2. **Phase 2**: Fine-tuning (optional)\n",
    "   - Unfreeze top layers of the backbone\n",
    "   - Lower learning rate for fine-tuning\n",
    "   - Additional 10-20 epochs\n",
    "\n",
    "## Callbacks and Monitoring\n",
    "- **EarlyStopping**: Prevent overfitting\n",
    "- **ReduceLROnPlateau**: Adaptive learning rate\n",
    "- **ModelCheckpoint**: Save best model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# üöÄ SMART TRAINING PIPELINE - TensorFlow 2.19\n",
    "# =======================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç CHECKING FOR EXISTING TRAINED MODEL...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Check if trained model exists\n",
    "model_path = Path('best_car_model_EfficientNetB4.keras')\n",
    "if model_path.exists():\n",
    "    print(f\"‚úÖ Found existing trained model: {model_path}\")\n",
    "    print(f\"üìä Model size: {model_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    try:\n",
    "        # Load the existing model\n",
    "        print(\"üîÑ Loading pre-trained model...\")\n",
    "        model = tf.keras.models.load_model(str(model_path))\n",
    "        print(\"‚úÖ Pre-trained model loaded successfully!\")\n",
    "        print(f\"üìä Model parameters: {model.count_params():,}\")\n",
    "        \n",
    "        # Verify model works (only if dataset is ready and val_ds exists)\n",
    "        print(\"üß™ Testing model...\")\n",
    "        if DATASET_READY and 'val_ds' in globals():\n",
    "            sample_batch = next(iter(val_ds.take(1)))\n",
    "            test_prediction = model.predict(sample_batch[0][:1], verbose=0)\n",
    "            print(f\"‚úÖ Model test successful - output shape: {test_prediction.shape}\")\n",
    "        else:\n",
    "            # Create a simple test with random data instead\n",
    "            test_input = tf.random.normal((1, 224, 224, 3))\n",
    "            test_prediction = model.predict(test_input, verbose=0)\n",
    "            print(f\"‚úÖ Model test successful - output shape: {test_prediction.shape}\")\n",
    "        \n",
    "        SKIP_TRAINING = True\n",
    "        print(\"\\nüéØ TRAINING WILL BE SKIPPED - Using existing model\")\n",
    "        print(\"üí° To retrain, delete 'best_car_model_EfficientNetB4.keras' and rerun this cell\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load existing model: {e}\")\n",
    "        print(\"üîß Will create and train new model instead...\")\n",
    "        SKIP_TRAINING = False\n",
    "\n",
    "else:\n",
    "    print(\"üìã No existing model found\")\n",
    "    print(\"üöÄ Will proceed with training new model\")\n",
    "    SKIP_TRAINING = False\n",
    "\n",
    "# =======================================\n",
    "# CONDITIONAL TRAINING LOGIC\n",
    "# =======================================\n",
    "\n",
    "if SKIP_TRAINING:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîÑ SKIPPING TRAINING - USING EXISTING MODEL\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"‚úÖ Model ready: {model.name}\")\n",
    "    print(f\"üìä Classes: {model.output_shape[-1]}\")\n",
    "    print(f\"üìê Input shape: {model.input_shape}\")\n",
    "    print(\"üéØ Proceed to evaluation section\")\n",
    "    \n",
    "elif not DATASET_READY:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚ö†Ô∏è SKIPPING TRAINING - NO DATASET AVAILABLE\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"‚ùå Cannot train without Stanford Cars dataset\")\n",
    "    print(\"üí° Please ensure dataset is downloaded and rerun\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üöÄ STARTING TRAINING WITH TENSORFLOW 2.19\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Check if we need to fix the model for TF 2.19 compatibility\n",
    "    if not hasattr(model, '_tf219_fixed'):\n",
    "        print(\"üîß Applying TensorFlow 2.19 compatibility fixes...\")\n",
    "        \n",
    "        # Get the current model's layers except the last one\n",
    "        layers_to_keep = model.layers[:-1]\n",
    "        \n",
    "        # Create a new final layer with explicit float32 dtype\n",
    "        final_layer = tf.keras.layers.Dense(\n",
    "            NUM_CLASSES,\n",
    "            activation='softmax',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            dtype='float32',  # This is the key fix for mixed precision\n",
    "            name='predictions_fixed'\n",
    "        )\n",
    "        \n",
    "        # Rebuild the model\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        model = Sequential(layers_to_keep + [final_layer])\n",
    "        \n",
    "        # Recompile with TF 2.19 compatible metrics\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy', dtype='float32')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Mark as fixed\n",
    "        model._tf219_fixed = True\n",
    "        print(\"‚úÖ Model fixed for TensorFlow 2.19 compatibility\")\n",
    "\n",
    "    # Create tf.data.Dataset from directory structure\n",
    "    print(\"üìÅ Setting up datasets...\")\n",
    "    \n",
    "    # Training dataset with augmentation\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode='categorical'\n",
    "    )\n",
    "\n",
    "    # Validation dataset\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\", \n",
    "        seed=123,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode='categorical'\n",
    "    )\n",
    "\n",
    "    # Test dataset\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TEST_DIR,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Optimize datasets for performance\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    print(f\"‚úÖ Datasets ready: train={len(train_ds)} batches, val={len(val_ds)} batches, test={len(test_ds)} batches\")\n",
    "    print(\"üéØ Using real Stanford Cars dataset\")\n",
    "\n",
    "    # Start training\n",
    "    print(\"\\nüèãÔ∏è STARTING TRAINING...\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"üìä Training on {NUM_CLASSES} car classes\")\n",
    "    print(f\"üéØ Max epochs: {EPOCHS} (early stopping enabled)\")\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ TRAINING COMPLETED!\")\n",
    "    print(\"üìä Training history saved to 'history' variable\")\n",
    "    print(\"üíæ Best model saved as 'best_car_model_EfficientNetB4.keras'\")\n",
    "    \n",
    "    # Save training history to file for persistence\n",
    "    print(\"üíæ Saving training history to file...\")\n",
    "    import pickle\n",
    "    import json\n",
    "    \n",
    "    try:\n",
    "        # Save as pickle file (preserves exact structure)\n",
    "        with open('training_history.pkl', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "        print(\"‚úÖ Training history saved as 'training_history.pkl'\")\n",
    "        \n",
    "        # Also save as JSON for readability (though less precise)\n",
    "        history_json = {}\n",
    "        for key, values in history.history.items():\n",
    "            history_json[key] = [float(v) for v in values]  # Convert to regular Python floats\n",
    "        \n",
    "        with open('training_history.json', 'w') as f:\n",
    "            json.dump(history_json, f, indent=2)\n",
    "        print(\"‚úÖ Training history also saved as 'training_history.json'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not save training history: {e}\")\n",
    "        print(\"üí° History is still available in 'history' variable for this session\")\n",
    "\n",
    "print(f\"\\nüìã Final Status: {'Training Skipped (model exists)' if SKIP_TRAINING else 'Training Completed' if DATASET_READY else 'Training Skipped (no dataset)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# LOAD TRAINING HISTORY (For Plotting)\n",
    "# =======================================\n",
    "\n",
    "def load_training_history():\n",
    "    \"\"\"\n",
    "    Load training history from saved files.\n",
    "    This ensures plotting works even after restarting the kernel.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training history or None if not found\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(\"üîÑ Loading training history...\")\n",
    "    \n",
    "    # Try to load from pickle file first (most accurate)\n",
    "    pickle_path = Path('training_history.pkl')\n",
    "    if pickle_path.exists():\n",
    "        try:\n",
    "            with open(pickle_path, 'rb') as f:\n",
    "                history_data = pickle.load(f)\n",
    "            print(\"‚úÖ Loaded training history from 'training_history.pkl'\")\n",
    "            return history_data\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not load pickle file: {e}\")\n",
    "    \n",
    "    # Fallback to JSON file\n",
    "    json_path = Path('training_history.json')\n",
    "    if json_path.exists():\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                history_data = json.load(f)\n",
    "            print(\"‚úÖ Loaded training history from 'training_history.json'\")\n",
    "            return history_data\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not load JSON file: {e}\")\n",
    "    \n",
    "    print(\"‚ùå No training history files found\")\n",
    "    print(\"üí° If you've trained the model, the history should be in the 'history' variable\")\n",
    "    return None\n",
    "\n",
    "# Load history if not already available\n",
    "if 'history' not in globals() or history is None:\n",
    "    print(\"üìä TRAINING HISTORY NOT FOUND IN MEMORY\")\n",
    "    print(\"üîç Attempting to load from saved files...\")\n",
    "    \n",
    "    loaded_history = load_training_history()\n",
    "    \n",
    "    if loaded_history:\n",
    "        # Create a mock history object with the loaded data\n",
    "        class MockHistory:\n",
    "            def __init__(self, history_dict):\n",
    "                self.history = history_dict\n",
    "        \n",
    "        history = MockHistory(loaded_history)\n",
    "        print(\"‚úÖ Training history restored successfully!\")\n",
    "        print(f\"üìä Available metrics: {list(history.history.keys())}\")\n",
    "        print(f\"üìà Training epochs: {len(history.history.get('loss', []))}\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not load training history\")\n",
    "        print(\"üí° You may need to train the model first\")\n",
    "        history = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚úÖ Training history already available in memory\")\n",
    "    if hasattr(history, 'history'):\n",
    "        print(f\"üìä Available metrics: {list(history.history.keys())}\")\n",
    "        print(f\"üìà Training epochs: {len(history.history.get('loss', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba71c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# TRAINING HISTORY MANAGEMENT UTILITIES\n",
    "# =======================================\n",
    "from datetime import datetime\n",
    "def save_current_history():\n",
    "    \"\"\"\n",
    "    Manually save the current training history to files.\n",
    "    Useful if you want to preserve history from fine-tuning or additional training.\n",
    "    \"\"\"\n",
    "    if 'history' not in globals() or history is None:\n",
    "        print(\"‚ùå No training history available to save\")\n",
    "        return False\n",
    "        \n",
    "    import pickle\n",
    "    import json\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    try:\n",
    "        # Save as pickle file (most accurate)\n",
    "        pickle_file = f'training_history_{timestamp}.pkl'\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "        \n",
    "        # Also save as main file\n",
    "        with open('training_history.pkl', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "        \n",
    "        print(f\"‚úÖ Training history saved as '{pickle_file}'\")\n",
    "        print(\"‚úÖ Training history updated in 'training_history.pkl'\")\n",
    "        \n",
    "        # Save as JSON for readability\n",
    "        history_json = {}\n",
    "        for key, values in history.history.items():\n",
    "            history_json[key] = [float(v) for v in values]\n",
    "        \n",
    "        json_file = f'training_history_{timestamp}.json'\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(history_json, f, indent=2)\n",
    "            \n",
    "        with open('training_history.json', 'w') as f:\n",
    "            json.dump(history_json, f, indent=2)\n",
    "            \n",
    "        print(f\"‚úÖ Training history also saved as '{json_file}'\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving training history: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_history_files():\n",
    "    \"\"\"Check what training history files are available\"\"\"\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    \n",
    "    print(\"üìÅ AVAILABLE TRAINING HISTORY FILES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check for main files\n",
    "    main_files = ['training_history.pkl', 'training_history.json']\n",
    "    for filename in main_files:\n",
    "        path = Path(filename)\n",
    "        if path.exists():\n",
    "            size = path.stat().st_size / 1024  # KB\n",
    "            mod_time = path.stat().st_mtime\n",
    "            mod_time_str = datetime.fromtimestamp(mod_time).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"‚úÖ {filename} ({size:.1f}KB, modified: {mod_time_str})\")\n",
    "        else:\n",
    "            print(f\"‚ùå {filename} - Not found\")\n",
    "    \n",
    "    # Check for timestamped files\n",
    "    pkl_files = list(Path('.').glob('training_history_*.pkl'))\n",
    "    json_files = list(Path('.').glob('training_history_*.json'))\n",
    "    \n",
    "    if pkl_files or json_files:\n",
    "        print(f\"\\nüìÖ TIMESTAMPED HISTORY FILES:\")\n",
    "        all_files = pkl_files + json_files\n",
    "        for file_path in sorted(all_files):\n",
    "            size = file_path.stat().st_size / 1024  # KB\n",
    "            mod_time = file_path.stat().st_mtime\n",
    "            mod_time_str = datetime.fromtimestamp(mod_time).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"   üìÑ {file_path.name} ({size:.1f}KB, {mod_time_str})\")\n",
    "    \n",
    "    print(f\"\\nüí° Use load_training_history() to load the main history file\")\n",
    "    print(f\"üíæ Use save_current_history() to save current history\")\n",
    "\n",
    "# Check what history files are currently available\n",
    "check_history_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2138c56",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation\n",
    "\n",
    "## Objective\n",
    "Evaluate the trained model on test data and compute comprehensive metrics.\n",
    "\n",
    "**Thought Process: With 196 classes, top-5 accuracy is important to assess if the model is close to correct. We'll evaluate on test set for final metrics, plot training history for trends, and analyze confusion matrix for common errors, expecting some confusion between similar car models.**\n",
    "\n",
    "## Evaluation Metrics\n",
    "- **Top-1 Accuracy**: Standard classification accuracy\n",
    "- **Top-5 Accuracy**: Accuracy if correct class is in top 5 predictions\n",
    "- **Confusion Matrix**: Detailed class-wise performance\n",
    "- **Classification Report**: Precision, Recall, F1-Score per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# PLOT TRAINING HISTORY (ANYTIME)\n",
    "# =======================================\n",
    "\n",
    "def plot_training_history(history_obj=None, save_plots=True):\n",
    "    \"\"\"\n",
    "    Plot training history with automatic loading if not provided.\n",
    "    This function works even after kernel restarts.\n",
    "    \n",
    "    Args:\n",
    "        history_obj: History object (if None, will try to load from files)\n",
    "        save_plots: Whether to save plots as image files\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Use provided history or try to load it\n",
    "    if history_obj is None:\n",
    "        if 'history' in globals() and history is not None:\n",
    "            history_obj = history\n",
    "        else:\n",
    "            print(\"üîÑ No history provided, attempting to load from files...\")\n",
    "            loaded_history = load_training_history()\n",
    "            if loaded_history:\n",
    "                class MockHistory:\n",
    "                    def __init__(self, history_dict):\n",
    "                        self.history = history_dict\n",
    "                history_obj = MockHistory(loaded_history)\n",
    "            else:\n",
    "                print(\"‚ùå No training history available for plotting\")\n",
    "                return False\n",
    "    \n",
    "    if not hasattr(history_obj, 'history') or not history_obj.history:\n",
    "        print(\"‚ùå Invalid history object\")\n",
    "        return False\n",
    "    \n",
    "    print(\"üìà PLOTTING TRAINING HISTORY\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Clear any existing plots\n",
    "    plt.clf()\n",
    "    plt.close('all')\n",
    "    \n",
    "    # Determine number of subplots based on available metrics\n",
    "    metrics = list(history_obj.history.keys())\n",
    "    has_top5 = any('top_5_accuracy' in key for key in metrics)\n",
    "    n_plots = 3 if has_top5 else 2\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 5))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    elif n_plots == 2:\n",
    "        axes = list(axes)\n",
    "    \n",
    "    try:\n",
    "        # Accuracy plot\n",
    "        if 'accuracy' in history_obj.history:\n",
    "            axes[0].plot(history_obj.history['accuracy'], label='Training', linewidth=2, color='blue')\n",
    "            if 'val_accuracy' in history_obj.history:\n",
    "                axes[0].plot(history_obj.history['val_accuracy'], label='Validation', linewidth=2, color='orange')\n",
    "            axes[0].set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "            axes[0].set_xlabel('Epoch')\n",
    "            axes[0].set_ylabel('Accuracy')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            axes[0].set_ylim([0, 1])\n",
    "            \n",
    "            # Add final accuracy text\n",
    "            final_train_acc = history_obj.history['accuracy'][-1]\n",
    "            final_val_acc = history_obj.history.get('val_accuracy', [0])[-1] if 'val_accuracy' in history_obj.history else 0\n",
    "            axes[0].text(0.02, 0.98, f'Final: Train={final_train_acc:.3f}, Val={final_val_acc:.3f}', \n",
    "                        transform=axes[0].transAxes, verticalalignment='top', \n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Loss plot\n",
    "        if 'loss' in history_obj.history:\n",
    "            axes[1].plot(history_obj.history['loss'], label='Training', linewidth=2, color='red')\n",
    "            if 'val_loss' in history_obj.history:\n",
    "                axes[1].plot(history_obj.history['val_loss'], label='Validation', linewidth=2, color='purple')\n",
    "            axes[1].set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel('Loss')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add final loss text\n",
    "            final_train_loss = history_obj.history['loss'][-1]\n",
    "            final_val_loss = history_obj.history.get('val_loss', [0])[-1] if 'val_loss' in history_obj.history else 0\n",
    "            axes[1].text(0.02, 0.98, f'Final: Train={final_train_loss:.3f}, Val={final_val_loss:.3f}', \n",
    "                        transform=axes[1].transAxes, verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Top-5 accuracy plot if available\n",
    "        if has_top5 and n_plots >= 3:\n",
    "            if 'top_5_accuracy' in history_obj.history:\n",
    "                axes[2].plot(history_obj.history['top_5_accuracy'], label='Training', linewidth=2, color='green')\n",
    "                if 'val_top_5_accuracy' in history_obj.history:\n",
    "                    axes[2].plot(history_obj.history['val_top_5_accuracy'], label='Validation', linewidth=2, color='brown')\n",
    "                axes[2].set_title('Top-5 Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "                axes[2].set_xlabel('Epoch')\n",
    "                axes[2].set_ylabel('Top-5 Accuracy')\n",
    "                axes[2].legend()\n",
    "                axes[2].grid(True, alpha=0.3)\n",
    "                axes[2].set_ylim([0, 1])\n",
    "                \n",
    "                # Add final top-5 accuracy text\n",
    "                final_train_top5 = history_obj.history['top_5_accuracy'][-1]\n",
    "                final_val_top5 = history_obj.history.get('val_top_5_accuracy', [0])[-1] if 'val_top_5_accuracy' in history_obj.history else 0\n",
    "                axes[2].text(0.02, 0.98, f'Final: Train={final_train_top5:.3f}, Val={final_val_top5:.3f}', \n",
    "                            transform=axes[2].transAxes, verticalalignment='top',\n",
    "                            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save plots if requested\n",
    "        if save_plots:\n",
    "            try:\n",
    "                from datetime import datetime\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                plot_filename = f'training_plots_{timestamp}.png'\n",
    "                fig.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "                \n",
    "                # Also save as main plot file\n",
    "                fig.savefig('training_plots.png', dpi=300, bbox_inches='tight')\n",
    "                print(f\"üíæ Plots saved as '{plot_filename}' and 'training_plots.png'\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not save plots: {e}\")\n",
    "        \n",
    "        # Display summary\n",
    "        epochs = len(history_obj.history.get('loss', []))\n",
    "        print(f\"\\nüìä TRAINING SUMMARY:\")\n",
    "        print(f\"   Total epochs: {epochs}\")\n",
    "        print(f\"   Available metrics: {', '.join(metrics)}\")\n",
    "        \n",
    "        plt.close(fig)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating plots: {e}\")\n",
    "        plt.close(fig)\n",
    "        return False\n",
    "\n",
    "# Demonstrate plotting functionality\n",
    "print(\"üé® TRAINING HISTORY PLOTTING READY!\")\n",
    "print(\"=\" * 35)\n",
    "print(\"üí° To plot training history anytime, use:\")\n",
    "print(\"   plot_training_history()  # Uses loaded history\")\n",
    "print(\"   plot_training_history(history)  # Uses specific history object\")\n",
    "print(\"\\nüîÑ Attempting to plot current history...\")\n",
    "\n",
    "if plot_training_history():\n",
    "    print(\"‚úÖ Training history plotted successfully!\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No training history available to plot yet\")\n",
    "    print(\"üèãÔ∏è  Train the model first, then this will work automatically\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# MODEL EVALUATION AND RESULTS\n",
    "# =======================================\n",
    "from matplotlib import pyplot as plt    \n",
    "\n",
    "print(\"üìä EVALUATING MODEL PERFORMANCE\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "if not DATASET_READY:\n",
    "    print(\"‚ö†Ô∏è Dataset not available - skipping evaluation\")\n",
    "    print(\"üí° Model is ready for inference with external data\")\n",
    "    print(f\"üìã Model summary:\")\n",
    "    print(f\"   Architecture: {model.name}\")\n",
    "    print(f\"   Parameters: {model.count_params():,}\")\n",
    "    print(f\"   Input shape: {model.input_shape}\")\n",
    "    print(f\"   Output classes: {model.output_shape[-1]}\")\n",
    "    \n",
    "else:\n",
    "    # Ensure we have the datasets available\n",
    "    if 'val_ds' not in globals() or 'test_ds' not in globals():\n",
    "        print(\"üîÑ Creating evaluation datasets...\")\n",
    "        \n",
    "        # Create datasets for evaluation\n",
    "        val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            TRAIN_DIR,\n",
    "            validation_split=0.2,\n",
    "            subset=\"validation\", \n",
    "            seed=123,\n",
    "            image_size=(224, 224),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            label_mode='categorical'\n",
    "        )\n",
    "\n",
    "        test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            TEST_DIR,\n",
    "            image_size=(224, 224),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            label_mode='categorical',\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Optimize for evaluation\n",
    "        val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(\"‚úÖ Evaluation datasets ready\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    print(\"üéØ Validation Set Results:\")\n",
    "    val_results = model.evaluate(val_ds, verbose=0, return_dict=True)\n",
    "    print(f\"   Loss: {val_results['loss']:.4f}\")\n",
    "    print(f\"   Accuracy: {val_results['accuracy']:.4f} ({val_results['accuracy']*100:.2f}%)\")\n",
    "    if 'top_5_accuracy' in val_results:\n",
    "        print(f\"   Top-5 Accuracy: {val_results['top_5_accuracy']:.4f} ({val_results['top_5_accuracy']*100:.2f}%)\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nüéØ Test Set Results:\")\n",
    "    test_results = model.evaluate(test_ds, verbose=0, return_dict=True)\n",
    "    print(f\"   Loss: {test_results['loss']:.4f}\")\n",
    "    print(f\"   Accuracy: {test_results['accuracy']:.4f} ({test_results['accuracy']*100:.2f}%)\")\n",
    "    if 'top_5_accuracy' in test_results:\n",
    "        print(f\"   Top-5 Accuracy: {test_results['top_5_accuracy']:.4f} ({test_results['top_5_accuracy']*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ede928",
   "metadata": {},
   "source": [
    "# 6. Model Export and Deployment Preparation\n",
    "\n",
    "## Objective\n",
    "Save the trained model in multiple formats and generate necessary deployment files for the API service.\n",
    "\n",
    "**Thought Process: We'll save in multiple formats for flexibility: H5 for legacy, .keras for native TensorFlow, and SavedModel for production. FastAPI is chosen for its modern features, and Docker ensures scalable deployment. Class mapping ensures predictions map to car names, crucial for user-facing API.**\n",
    "\n",
    "## Export Process\n",
    "- **H5 Format**: Compatible with existing FastAPI service\n",
    "- **SavedModel Format**: TensorFlow's recommended production format  \n",
    "- **Class Mapping**: JSON file mapping class indices to car type names\n",
    "- **API Compatibility**: Verification that saved model works with the API\n",
    "\n",
    "## Generated Files\n",
    "- `car_classification_model.h5` - H5 model (legacy format, for backward compatibility)\n",
    "- `best_car_model.keras` - Keras model (recommended native format)\n",
    "- `models/car_classification_savedmodel/` - SavedModel format for TF Serving/TF Lite\n",
    "- `class_mapping.json` - Class index to name mapping\n",
    "- `prediction_example.py` - Example usage code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# üìÅ MODEL EXPORT AND DEPLOYMENT PREPARATION\n",
    "# =======================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model_and_metadata():\n",
    "    \"\"\"\n",
    "    Save the trained model and generate necessary metadata for deployment\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üíæ Saving Model and Generating Deployment Files...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Ensure models directory exists\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # 1. Save as H5 format (compatible with existing API - but will show deprecation warning)\n",
    "    model_h5_path = 'car_classification_model.h5'\n",
    "    model.save(model_h5_path)\n",
    "    file_size = Path(model_h5_path).stat().st_size / (1024*1024)\n",
    "    print(f\"‚úÖ Model saved as H5: {model_h5_path} ({file_size:.1f} MB)\")\n",
    "    print(\"   ‚ö†Ô∏è H5 format is legacy - consider using .keras format for new projects\")\n",
    "    \n",
    "    # 1b. Save as native Keras format (recommended)\n",
    "    model_keras_path = 'best_car_model.keras'\n",
    "    model.save(model_keras_path)\n",
    "    keras_file_size = Path(model_keras_path).stat().st_size / (1024*1024)\n",
    "    print(f\"‚úÖ Model saved as Keras: {model_keras_path} ({keras_file_size:.1f} MB)\")\n",
    "    \n",
    "    # 2. Save as SavedModel format (for TensorFlow Serving/TF Lite)\n",
    "    savedmodel_path = 'models/car_classification_savedmodel'\n",
    "    try:\n",
    "        # Use model.export() for SavedModel format in newer TensorFlow versions\n",
    "        if hasattr(model, 'export'):\n",
    "            model.export(savedmodel_path)\n",
    "            print(f\"‚úÖ Model exported as SavedModel: {savedmodel_path}\")\n",
    "        else:\n",
    "            # Fallback for older TensorFlow versions\n",
    "            tf.saved_model.save(model, savedmodel_path)\n",
    "            print(f\"‚úÖ Model saved as SavedModel: {savedmodel_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è SavedModel export failed: {e}\")\n",
    "        print(\"   Continuing with other formats...\")\n",
    "    \n",
    "    # 3. Generate class mapping for the API\n",
    "    if DATASET_READY:\n",
    "        # Get class names from the dataset\n",
    "        # Since we're using tf.data.Dataset, we need to extract class names differently\n",
    "        if 'train_ds' in globals():\n",
    "            # Get class names from train_ds if available\n",
    "            class_names = train_ds.class_names if hasattr(train_ds, 'class_names') else None\n",
    "        else:\n",
    "            class_names = None\n",
    "            \n",
    "        if class_names is not None:\n",
    "            # Create mapping from class index to class name\n",
    "            class_mapping = {\n",
    "                'index_to_class': {str(i): name for i, name in enumerate(class_names)},\n",
    "                'class_to_index': {name: i for i, name in enumerate(class_names)}\n",
    "            }\n",
    "            print(f\"‚úÖ Using real Stanford Cars dataset class names\")\n",
    "        else:\n",
    "            # Fallback: try to get class names from directory structure\n",
    "            try:\n",
    "                train_dir = Path(TRAIN_DIR)\n",
    "                class_names = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n",
    "                class_mapping = {\n",
    "                    'index_to_class': {str(i): name for i, name in enumerate(class_names)},\n",
    "                    'class_to_index': {name: i for i, name in enumerate(class_names)}\n",
    "                }\n",
    "                print(f\"‚úÖ Extracted {len(class_names)} class names from directory structure\")\n",
    "            except:\n",
    "                # Ultimate fallback: generic class names\n",
    "                class_mapping = {\n",
    "                    'index_to_class': {str(i): f\"Car_Class_{i+1}\" for i in range(NUM_CLASSES)},\n",
    "                    'class_to_index': {f\"Car_Class_{i+1}\": i for i in range(NUM_CLASSES)}\n",
    "                }\n",
    "                print(f\"‚ö†Ô∏è Using generic class names (dataset structure not accessible)\")\n",
    "    else:\n",
    "        # Use demo class names for synthetic data\n",
    "        class_mapping = {\n",
    "            'index_to_class': {str(i): f\"Car_Class_{i+1}\" for i in range(NUM_CLASSES)},\n",
    "            'class_to_index': {f\"Car_Class_{i+1}\": i for i in range(NUM_CLASSES)}\n",
    "        }\n",
    "        print(f\"‚úÖ Using demo class names (synthetic dataset)\")\n",
    "    \n",
    "    # Save class mapping as JSON\n",
    "    class_mapping_path = 'class_mapping.json'\n",
    "    with open(class_mapping_path, 'w') as f:\n",
    "        json.dump(class_mapping, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Class mapping saved: {class_mapping_path}\")\n",
    "    print(f\"üìä Total classes: {len(class_mapping['index_to_class'])}\")\n",
    "    \n",
    "    # Display sample class mappings\n",
    "    print(f\"\\nüè∑Ô∏è Sample class mappings:\")\n",
    "    sample_classes = list(class_mapping['index_to_class'].items())[:10]\n",
    "    for idx, class_name in sample_classes:\n",
    "        print(f\"   {idx}: {class_name}\")\n",
    "    if len(class_mapping['index_to_class']) > 10:\n",
    "        print(f\"   ... and {len(class_mapping['index_to_class']) - 10} more classes\")\n",
    "    \n",
    "    return model_h5_path, model_keras_path, savedmodel_path, class_mapping_path\n",
    "\n",
    "def verify_api_compatibility():\n",
    "    \"\"\"\n",
    "    Verify that saved model is compatible with the existing API\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüîß Verifying API Compatibility...\")\n",
    "    \n",
    "    try:\n",
    "        # Test loading the model (like the API would)\n",
    "        loaded_model = tf.keras.models.load_model('car_classification_model.h5')\n",
    "        \n",
    "        # Test prediction with sample data matching expected input\n",
    "        test_input = np.random.rand(1, 224, 224, 3)  # Standard input size\n",
    "        prediction = loaded_model.predict(test_input, verbose=0)\n",
    "        \n",
    "        print(f\"‚úÖ Model loads successfully\")\n",
    "        print(f\"‚úÖ Prediction shape: {prediction.shape}\")\n",
    "        print(f\"‚úÖ Expected shape: (1, {NUM_CLASSES})\")\n",
    "        \n",
    "        # Verify class mapping\n",
    "        with open('class_mapping.json', 'r') as f:\n",
    "            loaded_mapping = json.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ Class mapping loads successfully\")\n",
    "        print(f\"‚úÖ Number of classes: {len(loaded_mapping['index_to_class'])}\")\n",
    "        \n",
    "        if prediction.shape[1] == len(loaded_mapping['index_to_class']):\n",
    "            print(\"‚úÖ Model output matches class mapping!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Warning: Model output doesn't match class mapping size\")\n",
    "        \n",
    "        # Test actual prediction pipeline\n",
    "        sample_prediction = np.argmax(prediction[0])\n",
    "        sample_class = loaded_mapping['index_to_class'][str(sample_prediction)]\n",
    "        print(f\"‚úÖ Sample prediction: Class {sample_prediction} -> {sample_class}\")\n",
    "        \n",
    "        print(\"üéØ Model is ready for deployment!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Compatibility check failed: {e}\")\n",
    "        print(\"üîß Please check model saving process\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Execute the model saving and preparation pipeline\n",
    "print(\"üöÄ STARTING MODEL EXPORT PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Save model and generate deployment files\n",
    "try:\n",
    "    model_files = save_model_and_metadata()\n",
    "    \n",
    "    # Verify API compatibility\n",
    "    compatibility_ok = verify_api_compatibility()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nüéâ Model Export Complete!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üìÅ Generated Files:\")\n",
    "    print(f\"   ‚úÖ {model_files[0]} - H5 model (legacy format)\")\n",
    "    print(f\"   ‚úÖ {model_files[1]} - Keras model (recommended)\")\n",
    "    print(f\"   ‚úÖ {model_files[2]} - SavedModel for production\")\n",
    "    print(f\"   ‚úÖ {model_files[3]} - Class mapping\")\n",
    "    print(f\"   ‚úÖ prediction_example.py - Usage example\")\n",
    "    \n",
    "    if compatibility_ok:\n",
    "        print(f\"\\nüöÄ Your model is ready for deployment!\")\n",
    "        print(\"üí° Next steps:\")\n",
    "        print(\"   1. Test the API: python run.py --mode local\")\n",
    "        print(\"   2. Upload an image: curl -X POST http://localhost:8000/predict -F 'image=@car_image.jpg'\")\n",
    "        print(\"   3. Deploy with Docker: python run.py --mode docker\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Please fix compatibility issues before deployment\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Model export failed: {e}\")\n",
    "    print(\"üîß Please check that the model variable is defined and training completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de8e9bb",
   "metadata": {},
   "source": [
    "# 7. Class Mapping and Model Export\n",
    "\n",
    "## Objective\n",
    "Create class mapping for the Stanford Cars dataset and export the final model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# üìã FINAL MODEL SUMMARY AND DEPLOYMENT\n",
    "# =======================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üìã FINAL MODEL SUMMARY\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Model information\n",
    "print(f\"üèóÔ∏è Architecture: ResNet50-based transfer learning\")\n",
    "print(f\"üìä Total parameters: {model.count_params():,}\")\n",
    "print(f\"üéØ Output classes: {model.output_shape[-1]}\")\n",
    "print(f\"üìê Input shape: {model.input_shape}\")\n",
    "print(f\"‚ö° TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üîß Mixed precision: {'Enabled' if tf.keras.mixed_precision.global_policy().name != 'float32' else 'Disabled'}\")\n",
    "\n",
    "# Check model files\n",
    "print(f\"\\nüíæ Model Files:\")\n",
    "model_files = {\n",
    "    'best_car_model.keras': 'Main model file (TensorFlow 2.19 format)',\n",
    "    'class_mapping.json': 'Class name mappings',\n",
    "}\n",
    "\n",
    "for filename, description in model_files.items():\n",
    "    file_path = Path(filename)\n",
    "    if file_path.exists():\n",
    "        size = file_path.stat().st_size / (1024*1024)  # MB\n",
    "        print(f\"   ‚úÖ {filename} ({size:.1f}MB) - {description}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {filename} - {description} (missing)\")\n",
    "\n",
    "# Create class mapping if dataset available and file doesn't exist\n",
    "if DATASET_READY and not Path('class_mapping.json').exists():\n",
    "    print(f\"\\nüìù Creating class mapping...\")\n",
    "    class_names = sorted([d.name for d in TRAIN_DIR.iterdir() if d.is_dir()])\n",
    "    class_mapping = {i: name for i, name in enumerate(class_names)}\n",
    "    \n",
    "    with open('class_mapping.json', 'w') as f:\n",
    "        json.dump(class_mapping, f, indent=2)\n",
    "    print(f\"‚úÖ Class mapping saved with {len(class_mapping)} classes\")\n",
    "\n",
    "# Performance summary (if evaluation was done)\n",
    "if DATASET_READY and 'test_results' in globals():\n",
    "    print(f\"\\nüéØ PERFORMANCE SUMMARY:\")\n",
    "    print(f\"   Test Accuracy: {test_results['accuracy']*100:.2f}%\")\n",
    "    if 'top_5_accuracy' in test_results:\n",
    "        print(f\"   Test Top-5 Accuracy: {test_results['top_5_accuracy']*100:.2f}%\")\n",
    "    print(f\"   Model Status: {'Production Ready' if test_results['accuracy'] > 0.7 else 'Needs Improvement'}\")\n",
    "\n",
    "# Deployment information\n",
    "print(f\"\\nüöÄ DEPLOYMENT INFORMATION:\")\n",
    "print(f\"   1. Load model: tf.keras.models.load_model('best_car_model.keras')\")\n",
    "print(f\"   2. Preprocess image: Resize to 224x224, normalize to [0,1]\")\n",
    "print(f\"   3. Predict: model.predict(preprocessed_image)\")\n",
    "print(f\"   4. Get class name: Use class_mapping.json for label lookup\")\n",
    "\n",
    "print(f\"\\nüì± API INTEGRATION:\")\n",
    "print(f\"   ‚Ä¢ FastAPI server: run_api.py or standalone_api.py\")\n",
    "print(f\"   ‚Ä¢ Docker deployment: docker-compose up\")\n",
    "print(f\"   ‚Ä¢ Model format: Compatible with TensorFlow Serving\")\n",
    "\n",
    "# Usage example\n",
    "print(f\"\\nüí° QUICK USAGE EXAMPLE:\")\n",
    "print(f\"\"\"\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('best_car_model.keras')\n",
    "\n",
    "# Load class mapping\n",
    "with open('class_mapping.json', 'r') as f:\n",
    "    class_mapping = json.load(f)\n",
    "\n",
    "# Preprocess image\n",
    "image = tf.keras.utils.load_img('car_image.jpg', target_size=(224, 224))\n",
    "image_array = tf.keras.utils.img_to_array(image)\n",
    "image_array = tf.expand_dims(image_array, 0) / 255.0\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(image_array)\n",
    "predicted_class_idx = np.argmax(predictions[0])\n",
    "predicted_class_name = class_mapping[str(predicted_class_idx)]\n",
    "confidence = predictions[0][predicted_class_idx]\n",
    "\n",
    "print(f\"Predicted: {{predicted_class_name}} ({{confidence:.2%}} confidence)\")\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n‚úÖ Car Type Classification System Ready!\")\n",
    "print(f\"üéâ Model successfully {'loaded from existing file' if SKIP_TRAINING else 'trained and saved'}\")\n",
    "\n",
    "# Display final model architecture summary\n",
    "print(f\"\\nüèóÔ∏è Model Architecture Summary:\")\n",
    "try:\n",
    "    model.summary()\n",
    "except:\n",
    "    print(\"Model summary not available - check model loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1bd5f",
   "metadata": {},
   "source": [
    "# 8. Environment Verification\n",
    "\n",
    "## Package Versions\n",
    "This section captures the exact package versions used for model training to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0cd281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment verification - capture package versions for reproducibility\n",
    "print(\"üì¶ Installed Package Versions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Get pip freeze output\n",
    "result = subprocess.run([sys.executable, '-m', 'pip', 'freeze'], \n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "# Display package versions\n",
    "if result.returncode == 0:\n",
    "    packages = result.stdout.strip().split('\\n')\n",
    "    # Show key packages first\n",
    "    key_packages = ['tensorflow', 'keras', 'numpy', 'pillow', 'fastapi', 'uvicorn']\n",
    "    \n",
    "    print(\"üîë Key Packages:\")\n",
    "    for package in packages:\n",
    "        package_name = package.split('==')[0].lower()\n",
    "        if any(key in package_name for key in key_packages):\n",
    "            print(f\"  {package}\")\n",
    "    \n",
    "    print(f\"\\nüìã All Packages ({len(packages)} total):\")\n",
    "    for package in packages:\n",
    "        print(f\"  {package}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error running pip freeze: {result.stderr}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Environment verification complete!\")\n",
    "print(f\"üí° These versions should match requirements.txt for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10ba2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
